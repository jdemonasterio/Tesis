{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A partir de una tabla de datos ya procesada para utilizar con los modelos teoricos, \n",
    "en este nb buscamos optimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "## seteamos los parametros del notebook\n",
    "%autosave 180\n",
    "\n",
    "import pandas as pd; \n",
    "import numpy as np; \n",
    "import os;\n",
    "import random;\n",
    "import time\n",
    "#import matplotlib\n",
    "#%matplotlib inline\n",
    "import sys\n",
    "\n",
    "np.random.seed(2015)\n",
    "rootdir=os.getcwd()\n",
    "rootdir+= \"/datasets\"\n",
    "os.chdir(rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from unbalanced_dataset import UnderSampler, NearMiss, CondensedNearestNeighbour, OneSidedSelection,\\\n",
    "#NeighbourhoodCleaningRule, TomekLinks, ClusterCentroids, OverSampler, SMOTE,\\\n",
    "#SMOTETomek, SMOTEENN, EasyEnsemble, BalanceCascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31mground_truth.csv.gz\u001b[0m  \u001b[01;31msf_output.txt.gz\u001b[0m  \u001b[01;31msl_output.txt.gz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls *.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER|ANTENNA_ID_0|COUNT_0|ANTENNA_ID_1|COUNT_1|ANTENNA_ID_2|COUNT_2|ANTENNA_ID_3|COUNT_3|ANTENNA_ID_4|COUNT_4|ANTENNA_ID_5|COUNT_5|ANTENNA_ID_6|COUNT_6|ANTENNA_ID_7|COUNT_7|ANTENNA_ID_8|COUNT_8|ANTENNA_ID_9|COUNT_9|MOBILITY_DIAMETER|EXPOSED|EPIDEMIC\r\n",
      "FFFF78CB080636632B2D1DE7A7BFAB03|3415|21|2969|5|1320|3|2471|3|1115|2|-1|65535|-1|65535|-1|65535|-1|65535|-1|65535|1063|False|False\r\n",
      "FFFF51FDDD55A2E24D074DE30C4798E9|1207|15|1171|14|1820|9|583|8|2026|5|1066|4|1831|4|1625|2|401|1|825|1|520|True|False\r\n",
      "FFFE1EDDA25369CBC467536FA6A787FD|1847|22|1753|14|1422|11|1028|10|765|9|3072|9|862|8|2463|7|1416|6|871|5|990|True|False\r\n",
      "FFFD0A912A959CDFCFFAB2F93BFBA435|1407|18|1702|8|1613|6|1704|5|1394|3|1398|3|818|2|958|2|726|1|848|1|491|True|False\r\n",
      "FFFCB235D6D917DDF350F5B11CCB0EE8|17|74|174|33|1123|4|1178|4|173|3|890|2|503|1|1039|1|1228|1|1327|1|1613|True|False\r\n",
      "FFFC9EC4F662C44D3006BA69C9F0DE85|876|19|1737|6|1744|5|326|1|378|1|503|1|1118|1|2216|1|2477|1|2962|1|3004|True|True\r\n",
      "FFFB2F18F4CCF253C06B581EA518A7A5|2543|13|704|7|2540|7|1453|6|1642|5|45|4|2393|4|1841|3|3031|3|1009|2|2895|True|False\r\n",
      "FFFA147CB2B23AD12F207F0EFBEB14D0|1857|103|2959|19|1344|2|2309|2|1511|1|2858|1|-1|65535|-1|65535|-1|65535|-1|65535|1633|True|True\r\n",
      "FFFA0736F66FB09200CE1D13E081A946|266|63|1799|46|1790|29|2716|29|537|21|3377|21|1439|18|2715|16|1720|15|2000|14|2558|True|True\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zcat sf_output.txt.gz| head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File: `sf_output.txt.gz'\r\n",
      "  Size: 181994072 \tBlocks: 355464     IO Block: 4096   regular file\r\n",
      "Device: 831h/2097d\tInode: 68164921    Links: 1\r\n",
      "Access: (0644/-rw-r--r--)  Uid: ( 3001/    juan)   Gid: ( 3001/    juan)\r\n",
      "Access: 2016-06-08 14:35:07.115044438 +0000\r\n",
      "Modify: 2016-05-13 00:07:52.000000000 +0000\r\n",
      "Change: 2016-06-08 14:35:07.115044438 +0000\r\n",
      " Birth: -\r\n"
     ]
    }
   ],
   "source": [
    "!stat sf_output.txt.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_sf = \"sf_output.txt.gz\"\n",
    "file_sl = \"sl_output.txt.gz\"\n",
    "file_gt = \"ground_truth.csv.gz\"\n",
    "\n",
    "sf_table = pd.read_csv(file_sf,sep = \"|\",\n",
    "                   header =0,\n",
    "                     index_col =0)\n",
    "\n",
    "sl_table = pd.read_csv(file_sl,sep = \"|\",\n",
    "                   header =0,\n",
    "                     index_col =0)\n",
    "\n",
    "gt_table = pd.read_csv(file_gt,sep = \"|\",\n",
    "                   header =0,\n",
    "                     index_col =0)\n",
    "\n",
    "#agrego la etiqueta \"_gt\" a las columnas del ground_truth\n",
    "gt_table.columns = [col+\"_gt\" for col in gt_table.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3667451, 23), (2449242, 15), (2000285, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf_table.shape, sl_table.shape, gt_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000285"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking table consistency\n",
    "np.in1d(gt_table.index.values,sf_table.index.values).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000285"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.in1d(gt_table.index.values,sl_table.index.values).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANTENNA_ID_0</th>\n",
       "      <th>COUNT_0</th>\n",
       "      <th>ANTENNA_ID_1</th>\n",
       "      <th>COUNT_1</th>\n",
       "      <th>ANTENNA_ID_2</th>\n",
       "      <th>COUNT_2</th>\n",
       "      <th>ANTENNA_ID_3</th>\n",
       "      <th>COUNT_3</th>\n",
       "      <th>ANTENNA_ID_4</th>\n",
       "      <th>COUNT_4</th>\n",
       "      <th>...</th>\n",
       "      <th>COUNT_6</th>\n",
       "      <th>ANTENNA_ID_7</th>\n",
       "      <th>COUNT_7</th>\n",
       "      <th>ANTENNA_ID_8</th>\n",
       "      <th>COUNT_8</th>\n",
       "      <th>ANTENNA_ID_9</th>\n",
       "      <th>COUNT_9</th>\n",
       "      <th>MOBILITY_DIAMETER</th>\n",
       "      <th>EXPOSED</th>\n",
       "      <th>EPIDEMIC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01737C4634D44B15BDE3B4D1704B7A86</th>\n",
       "      <td>763</td>\n",
       "      <td>3</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "      <td>481</td>\n",
       "      <td>1</td>\n",
       "      <td>3444</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>...</td>\n",
       "      <td>65535</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F94E1DDF50F2FED57BB9F49C5082F612</th>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>2181</td>\n",
       "      <td>9</td>\n",
       "      <td>660</td>\n",
       "      <td>6</td>\n",
       "      <td>1743</td>\n",
       "      <td>5</td>\n",
       "      <td>2098</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>1603</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ED73ACA101DF2A0722B321BAC792A361</th>\n",
       "      <td>3694</td>\n",
       "      <td>21</td>\n",
       "      <td>821</td>\n",
       "      <td>17</td>\n",
       "      <td>3454</td>\n",
       "      <td>17</td>\n",
       "      <td>3549</td>\n",
       "      <td>17</td>\n",
       "      <td>446</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>815</td>\n",
       "      <td>12</td>\n",
       "      <td>568</td>\n",
       "      <td>11</td>\n",
       "      <td>751</td>\n",
       "      <td>9</td>\n",
       "      <td>1820</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2EC7962039DC318369B17C105D020CA</th>\n",
       "      <td>642</td>\n",
       "      <td>18</td>\n",
       "      <td>1924</td>\n",
       "      <td>8</td>\n",
       "      <td>2011</td>\n",
       "      <td>7</td>\n",
       "      <td>1596</td>\n",
       "      <td>4</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1396</td>\n",
       "      <td>2</td>\n",
       "      <td>2091</td>\n",
       "      <td>2</td>\n",
       "      <td>2832</td>\n",
       "      <td>2</td>\n",
       "      <td>1334</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ED1F4E5A278E80E1C70329928EAB4F82</th>\n",
       "      <td>271</td>\n",
       "      <td>19</td>\n",
       "      <td>2659</td>\n",
       "      <td>11</td>\n",
       "      <td>401</td>\n",
       "      <td>4</td>\n",
       "      <td>398</td>\n",
       "      <td>3</td>\n",
       "      <td>270</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>592</td>\n",
       "      <td>1</td>\n",
       "      <td>653</td>\n",
       "      <td>1</td>\n",
       "      <td>993</td>\n",
       "      <td>1</td>\n",
       "      <td>1148</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594979EB2DBCB0BCD8815BD2D5180D7E</th>\n",
       "      <td>2237</td>\n",
       "      <td>26</td>\n",
       "      <td>1916</td>\n",
       "      <td>17</td>\n",
       "      <td>1908</td>\n",
       "      <td>14</td>\n",
       "      <td>1973</td>\n",
       "      <td>9</td>\n",
       "      <td>1978</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>394</td>\n",
       "      <td>2</td>\n",
       "      <td>419</td>\n",
       "      <td>1</td>\n",
       "      <td>490</td>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25C009AE4C33CB348C1B86ED0B494FE8</th>\n",
       "      <td>2182</td>\n",
       "      <td>31</td>\n",
       "      <td>148</td>\n",
       "      <td>24</td>\n",
       "      <td>1724</td>\n",
       "      <td>21</td>\n",
       "      <td>2443</td>\n",
       "      <td>19</td>\n",
       "      <td>2033</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1674</td>\n",
       "      <td>4</td>\n",
       "      <td>2487</td>\n",
       "      <td>4</td>\n",
       "      <td>1905</td>\n",
       "      <td>3</td>\n",
       "      <td>1310</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F13667C6DAA7D4E5D2FCA6C62CFCEA7E</th>\n",
       "      <td>3558</td>\n",
       "      <td>44</td>\n",
       "      <td>3166</td>\n",
       "      <td>16</td>\n",
       "      <td>1341</td>\n",
       "      <td>15</td>\n",
       "      <td>832</td>\n",
       "      <td>6</td>\n",
       "      <td>997</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>794</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25FCA8BFC511E0A9011EBFCD184EC3AB</th>\n",
       "      <td>3142</td>\n",
       "      <td>9</td>\n",
       "      <td>416</td>\n",
       "      <td>7</td>\n",
       "      <td>3112</td>\n",
       "      <td>6</td>\n",
       "      <td>3141</td>\n",
       "      <td>4</td>\n",
       "      <td>3210</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3060</td>\n",
       "      <td>3</td>\n",
       "      <td>3102</td>\n",
       "      <td>3</td>\n",
       "      <td>1548</td>\n",
       "      <td>2</td>\n",
       "      <td>1945</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E7077E8007ECD9CBC1F79BB83D582CB0</th>\n",
       "      <td>2858</td>\n",
       "      <td>4</td>\n",
       "      <td>3080</td>\n",
       "      <td>2</td>\n",
       "      <td>2218</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>...</td>\n",
       "      <td>65535</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>-1</td>\n",
       "      <td>65535</td>\n",
       "      <td>1242</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ANTENNA_ID_0  COUNT_0  ANTENNA_ID_1  \\\n",
       "USER                                                                    \n",
       "01737C4634D44B15BDE3B4D1704B7A86           763        3           350   \n",
       "F94E1DDF50F2FED57BB9F49C5082F612            31       10          2181   \n",
       "ED73ACA101DF2A0722B321BAC792A361          3694       21           821   \n",
       "D2EC7962039DC318369B17C105D020CA           642       18          1924   \n",
       "ED1F4E5A278E80E1C70329928EAB4F82           271       19          2659   \n",
       "594979EB2DBCB0BCD8815BD2D5180D7E          2237       26          1916   \n",
       "25C009AE4C33CB348C1B86ED0B494FE8          2182       31           148   \n",
       "F13667C6DAA7D4E5D2FCA6C62CFCEA7E          3558       44          3166   \n",
       "25FCA8BFC511E0A9011EBFCD184EC3AB          3142        9           416   \n",
       "E7077E8007ECD9CBC1F79BB83D582CB0          2858        4          3080   \n",
       "\n",
       "                                  COUNT_1  ANTENNA_ID_2  COUNT_2  \\\n",
       "USER                                                               \n",
       "01737C4634D44B15BDE3B4D1704B7A86        1           481        1   \n",
       "F94E1DDF50F2FED57BB9F49C5082F612        9           660        6   \n",
       "ED73ACA101DF2A0722B321BAC792A361       17          3454       17   \n",
       "D2EC7962039DC318369B17C105D020CA        8          2011        7   \n",
       "ED1F4E5A278E80E1C70329928EAB4F82       11           401        4   \n",
       "594979EB2DBCB0BCD8815BD2D5180D7E       17          1908       14   \n",
       "25C009AE4C33CB348C1B86ED0B494FE8       24          1724       21   \n",
       "F13667C6DAA7D4E5D2FCA6C62CFCEA7E       16          1341       15   \n",
       "25FCA8BFC511E0A9011EBFCD184EC3AB        7          3112        6   \n",
       "E7077E8007ECD9CBC1F79BB83D582CB0        2          2218        1   \n",
       "\n",
       "                                  ANTENNA_ID_3  COUNT_3  ANTENNA_ID_4  \\\n",
       "USER                                                                    \n",
       "01737C4634D44B15BDE3B4D1704B7A86          3444        1            -1   \n",
       "F94E1DDF50F2FED57BB9F49C5082F612          1743        5          2098   \n",
       "ED73ACA101DF2A0722B321BAC792A361          3549       17           446   \n",
       "D2EC7962039DC318369B17C105D020CA          1596        4          2025   \n",
       "ED1F4E5A278E80E1C70329928EAB4F82           398        3           270   \n",
       "594979EB2DBCB0BCD8815BD2D5180D7E          1973        9          1978   \n",
       "25C009AE4C33CB348C1B86ED0B494FE8          2443       19          2033   \n",
       "F13667C6DAA7D4E5D2FCA6C62CFCEA7E           832        6           997   \n",
       "25FCA8BFC511E0A9011EBFCD184EC3AB          3141        4          3210   \n",
       "E7077E8007ECD9CBC1F79BB83D582CB0            -1    65535            -1   \n",
       "\n",
       "                                  COUNT_4    ...     COUNT_6  ANTENNA_ID_7  \\\n",
       "USER                                         ...                             \n",
       "01737C4634D44B15BDE3B4D1704B7A86    65535    ...       65535            -1   \n",
       "F94E1DDF50F2FED57BB9F49C5082F612        3    ...           1            67   \n",
       "ED73ACA101DF2A0722B321BAC792A361       16    ...          12           815   \n",
       "D2EC7962039DC318369B17C105D020CA        3    ...           2          1396   \n",
       "ED1F4E5A278E80E1C70329928EAB4F82        2    ...           1           592   \n",
       "594979EB2DBCB0BCD8815BD2D5180D7E        7    ...           3           394   \n",
       "25C009AE4C33CB348C1B86ED0B494FE8       11    ...           7          1674   \n",
       "F13667C6DAA7D4E5D2FCA6C62CFCEA7E        1    ...           1          2018   \n",
       "25FCA8BFC511E0A9011EBFCD184EC3AB        4    ...           3          3060   \n",
       "E7077E8007ECD9CBC1F79BB83D582CB0    65535    ...       65535            -1   \n",
       "\n",
       "                                  COUNT_7  ANTENNA_ID_8  COUNT_8  \\\n",
       "USER                                                               \n",
       "01737C4634D44B15BDE3B4D1704B7A86    65535            -1    65535   \n",
       "F94E1DDF50F2FED57BB9F49C5082F612        1            99        1   \n",
       "ED73ACA101DF2A0722B321BAC792A361       12           568       11   \n",
       "D2EC7962039DC318369B17C105D020CA        2          2091        2   \n",
       "ED1F4E5A278E80E1C70329928EAB4F82        1           653        1   \n",
       "594979EB2DBCB0BCD8815BD2D5180D7E        2           419        1   \n",
       "25C009AE4C33CB348C1B86ED0B494FE8        4          2487        4   \n",
       "F13667C6DAA7D4E5D2FCA6C62CFCEA7E        1            -1    65535   \n",
       "25FCA8BFC511E0A9011EBFCD184EC3AB        3          3102        3   \n",
       "E7077E8007ECD9CBC1F79BB83D582CB0    65535            -1    65535   \n",
       "\n",
       "                                  ANTENNA_ID_9  COUNT_9  MOBILITY_DIAMETER  \\\n",
       "USER                                                                         \n",
       "01737C4634D44B15BDE3B4D1704B7A86            -1    65535               2015   \n",
       "F94E1DDF50F2FED57BB9F49C5082F612           235        1               1603   \n",
       "ED73ACA101DF2A0722B321BAC792A361           751        9               1820   \n",
       "D2EC7962039DC318369B17C105D020CA          2832        2               1334   \n",
       "ED1F4E5A278E80E1C70329928EAB4F82           993        1               1148   \n",
       "594979EB2DBCB0BCD8815BD2D5180D7E           490        1               1600   \n",
       "25C009AE4C33CB348C1B86ED0B494FE8          1905        3               1310   \n",
       "F13667C6DAA7D4E5D2FCA6C62CFCEA7E            -1    65535                794   \n",
       "25FCA8BFC511E0A9011EBFCD184EC3AB          1548        2               1945   \n",
       "E7077E8007ECD9CBC1F79BB83D582CB0            -1    65535               1242   \n",
       "\n",
       "                                  EXPOSED  EPIDEMIC  \n",
       "USER                                                 \n",
       "01737C4634D44B15BDE3B4D1704B7A86    False     False  \n",
       "F94E1DDF50F2FED57BB9F49C5082F612     True      True  \n",
       "ED73ACA101DF2A0722B321BAC792A361     True     False  \n",
       "D2EC7962039DC318369B17C105D020CA     True      True  \n",
       "ED1F4E5A278E80E1C70329928EAB4F82     True     False  \n",
       "594979EB2DBCB0BCD8815BD2D5180D7E     True     False  \n",
       "25C009AE4C33CB348C1B86ED0B494FE8     True     False  \n",
       "F13667C6DAA7D4E5D2FCA6C62CFCEA7E     True      True  \n",
       "25FCA8BFC511E0A9011EBFCD184EC3AB     True     False  \n",
       "E7077E8007ECD9CBC1F79BB83D582CB0    False     False  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf_table.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CallsWeekDaylight</th>\n",
       "      <th>CallsWeekNight</th>\n",
       "      <th>CallsWeekend</th>\n",
       "      <th>TimeWeekDaylight</th>\n",
       "      <th>TimeWeekNight</th>\n",
       "      <th>TimeWeekend</th>\n",
       "      <th>CallsWeekDaylight_EPI</th>\n",
       "      <th>CallsWeekNight_EPI</th>\n",
       "      <th>CallsWeekend_EPI</th>\n",
       "      <th>TimeWeekDaylight_EPI</th>\n",
       "      <th>TimeWeekNight_EPI</th>\n",
       "      <th>TimeWeekend_EPI</th>\n",
       "      <th>TOTAL_USERS</th>\n",
       "      <th>EPI_USERS</th>\n",
       "      <th>EXP_USERS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LineKeyOrigin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000004F96010462FA7A71C7784E27476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000007EEE7895E17937CFF86379C8E17</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000837B868F926F67BD440A0845C1D</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>160</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000C5D3BFBEBD919F6F68386C3DD5E</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000129F4191FFD0963774D2307C7510</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  CallsWeekDaylight  CallsWeekNight  \\\n",
       "LineKeyOrigin                                                         \n",
       "000004F96010462FA7A71C7784E27476                  0               0   \n",
       "000007EEE7895E17937CFF86379C8E17                  2               0   \n",
       "00000837B868F926F67BD440A0845C1D                  1               6   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E                  4               0   \n",
       "0000129F4191FFD0963774D2307C7510                  2               0   \n",
       "\n",
       "                                  CallsWeekend  TimeWeekDaylight  \\\n",
       "LineKeyOrigin                                                      \n",
       "000004F96010462FA7A71C7784E27476             1                 0   \n",
       "000007EEE7895E17937CFF86379C8E17             0               144   \n",
       "00000837B868F926F67BD440A0845C1D             3                47   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E             0               281   \n",
       "0000129F4191FFD0963774D2307C7510             0                66   \n",
       "\n",
       "                                  TimeWeekNight  TimeWeekend  \\\n",
       "LineKeyOrigin                                                  \n",
       "000004F96010462FA7A71C7784E27476              0           34   \n",
       "000007EEE7895E17937CFF86379C8E17              0            0   \n",
       "00000837B868F926F67BD440A0845C1D            160          109   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E              0            0   \n",
       "0000129F4191FFD0963774D2307C7510              0            0   \n",
       "\n",
       "                                  CallsWeekDaylight_EPI  CallsWeekNight_EPI  \\\n",
       "LineKeyOrigin                                                                 \n",
       "000004F96010462FA7A71C7784E27476                      0                   0   \n",
       "000007EEE7895E17937CFF86379C8E17                      2                   0   \n",
       "00000837B868F926F67BD440A0845C1D                      0                   0   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E                      0                   0   \n",
       "0000129F4191FFD0963774D2307C7510                      0                   0   \n",
       "\n",
       "                                  CallsWeekend_EPI  TimeWeekDaylight_EPI  \\\n",
       "LineKeyOrigin                                                              \n",
       "000004F96010462FA7A71C7784E27476                 0                     0   \n",
       "000007EEE7895E17937CFF86379C8E17                 0                   144   \n",
       "00000837B868F926F67BD440A0845C1D                 0                     0   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E                 0                     0   \n",
       "0000129F4191FFD0963774D2307C7510                 0                     0   \n",
       "\n",
       "                                  TimeWeekNight_EPI  TimeWeekend_EPI  \\\n",
       "LineKeyOrigin                                                          \n",
       "000004F96010462FA7A71C7784E27476                  0                0   \n",
       "000007EEE7895E17937CFF86379C8E17                  0                0   \n",
       "00000837B868F926F67BD440A0845C1D                  0                0   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E                  0                0   \n",
       "0000129F4191FFD0963774D2307C7510                  0                0   \n",
       "\n",
       "                                  TOTAL_USERS  EPI_USERS  EXP_USERS  \n",
       "LineKeyOrigin                                                        \n",
       "000004F96010462FA7A71C7784E27476            1          0          1  \n",
       "000007EEE7895E17937CFF86379C8E17            1          1          1  \n",
       "00000837B868F926F67BD440A0845C1D            3          0          2  \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E            1          0          0  \n",
       "0000129F4191FFD0963774D2307C7510            2          0          2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANTENNA_ID_gt</th>\n",
       "      <th>EPIDEMIC_gt</th>\n",
       "      <th>EXPOSED_gt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FFFF51FDDD55A2E24D074DE30C4798E9</th>\n",
       "      <td>470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFFC3133FAEF588B9FA75487C864F774</th>\n",
       "      <td>1036</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFFB2F18F4CCF253C06B581EA518A7A5</th>\n",
       "      <td>497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFFB2C0FEE677EE733DDD85F11C5A14C</th>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFFB27D77413E57C38D14D876C3DE8CF</th>\n",
       "      <td>2946</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ANTENNA_ID_gt  EPIDEMIC_gt  EXPOSED_gt\n",
       "USER                                                                    \n",
       "FFFF51FDDD55A2E24D074DE30C4798E9            470            0           0\n",
       "FFFC3133FAEF588B9FA75487C864F774           1036            0           1\n",
       "FFFB2F18F4CCF253C06B581EA518A7A5            497            1           1\n",
       "FFFB2C0FEE677EE733DDD85F11C5A14C            214            0           1\n",
       "FFFB27D77413E57C38D14D876C3DE8CF           2946            0           1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para poder usar el estimador MultionmialNB, paso todos los valores negativos en las antenna_ids de sf_table a 0\n",
    "for col in [col for col in sf_table.columns.values if \"_ID\" in col]:\n",
    "    sf_table[col] = sf_table[col].replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([gt_table,sf_table, sl_table], axis=1, join='inner')\n",
    "y_train = X_train[[col for col in X_train.columns if \"_gt\" in col]].copy()\n",
    "X_train = X_train[[col for col in X_train.columns if not (\"_gt\" in col)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing different fitting objectives\n",
    "We have enough information to attack the following questions:\n",
    "* Predict if user A used to be exposed.\n",
    "* Predict if user A used to be epidemic.\n",
    "* Predict if user A used to have Antenna N as a base one. (antenna number has to be given and given that we have more than 4500 antenna, classes will be EXTREMELY unbalanced in this scenario )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_target = gt_table['EPIDEMIC_gt'].values\n",
    "#y_target = gt_table['EXPOSED_gt'].values\n",
    "X_train['y'] = y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del sf_table, sl_table, gt_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANTENNA_ID_0</th>\n",
       "      <th>COUNT_0</th>\n",
       "      <th>ANTENNA_ID_1</th>\n",
       "      <th>COUNT_1</th>\n",
       "      <th>ANTENNA_ID_2</th>\n",
       "      <th>COUNT_2</th>\n",
       "      <th>ANTENNA_ID_3</th>\n",
       "      <th>COUNT_3</th>\n",
       "      <th>ANTENNA_ID_4</th>\n",
       "      <th>COUNT_4</th>\n",
       "      <th>...</th>\n",
       "      <th>CallsWeekDaylight_EPI</th>\n",
       "      <th>CallsWeekNight_EPI</th>\n",
       "      <th>CallsWeekend_EPI</th>\n",
       "      <th>TimeWeekDaylight_EPI</th>\n",
       "      <th>TimeWeekNight_EPI</th>\n",
       "      <th>TimeWeekend_EPI</th>\n",
       "      <th>TOTAL_USERS</th>\n",
       "      <th>EPI_USERS</th>\n",
       "      <th>EXP_USERS</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000004F96010462FA7A71C7784E27476</th>\n",
       "      <td>2937</td>\n",
       "      <td>36</td>\n",
       "      <td>1787</td>\n",
       "      <td>22</td>\n",
       "      <td>3314</td>\n",
       "      <td>19</td>\n",
       "      <td>2919</td>\n",
       "      <td>3</td>\n",
       "      <td>3056</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000007EEE7895E17937CFF86379C8E17</th>\n",
       "      <td>625</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>5</td>\n",
       "      <td>624</td>\n",
       "      <td>3</td>\n",
       "      <td>1507</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000837B868F926F67BD440A0845C1D</th>\n",
       "      <td>650</td>\n",
       "      <td>102</td>\n",
       "      <td>1955</td>\n",
       "      <td>48</td>\n",
       "      <td>1915</td>\n",
       "      <td>26</td>\n",
       "      <td>2037</td>\n",
       "      <td>3</td>\n",
       "      <td>1649</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000C5D3BFBEBD919F6F68386C3DD5E</th>\n",
       "      <td>1325</td>\n",
       "      <td>3</td>\n",
       "      <td>2699</td>\n",
       "      <td>1</td>\n",
       "      <td>3227</td>\n",
       "      <td>1</td>\n",
       "      <td>3434</td>\n",
       "      <td>1</td>\n",
       "      <td>3583</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000129F4191FFD0963774D2307C7510</th>\n",
       "      <td>2226</td>\n",
       "      <td>18</td>\n",
       "      <td>306</td>\n",
       "      <td>12</td>\n",
       "      <td>891</td>\n",
       "      <td>11</td>\n",
       "      <td>169</td>\n",
       "      <td>10</td>\n",
       "      <td>364</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ANTENNA_ID_0  COUNT_0  ANTENNA_ID_1  \\\n",
       "000004F96010462FA7A71C7784E27476          2937       36          1787   \n",
       "000007EEE7895E17937CFF86379C8E17           625        6            33   \n",
       "00000837B868F926F67BD440A0845C1D           650      102          1955   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E          1325        3          2699   \n",
       "0000129F4191FFD0963774D2307C7510          2226       18           306   \n",
       "\n",
       "                                  COUNT_1  ANTENNA_ID_2  COUNT_2  \\\n",
       "000004F96010462FA7A71C7784E27476       22          3314       19   \n",
       "000007EEE7895E17937CFF86379C8E17        5          1915        5   \n",
       "00000837B868F926F67BD440A0845C1D       48          1915       26   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E        1          3227        1   \n",
       "0000129F4191FFD0963774D2307C7510       12           891       11   \n",
       "\n",
       "                                  ANTENNA_ID_3  COUNT_3  ANTENNA_ID_4  \\\n",
       "000004F96010462FA7A71C7784E27476          2919        3          3056   \n",
       "000007EEE7895E17937CFF86379C8E17           624        3          1507   \n",
       "00000837B868F926F67BD440A0845C1D          2037        3          1649   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E          3434        1          3583   \n",
       "0000129F4191FFD0963774D2307C7510           169       10           364   \n",
       "\n",
       "                                  COUNT_4 ...  CallsWeekDaylight_EPI  \\\n",
       "000004F96010462FA7A71C7784E27476        1 ...                      0   \n",
       "000007EEE7895E17937CFF86379C8E17        2 ...                      2   \n",
       "00000837B868F926F67BD440A0845C1D        2 ...                      0   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E        1 ...                      0   \n",
       "0000129F4191FFD0963774D2307C7510       10 ...                      0   \n",
       "\n",
       "                                  CallsWeekNight_EPI  CallsWeekend_EPI  \\\n",
       "000004F96010462FA7A71C7784E27476                   0                 0   \n",
       "000007EEE7895E17937CFF86379C8E17                   0                 0   \n",
       "00000837B868F926F67BD440A0845C1D                   0                 0   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E                   0                 0   \n",
       "0000129F4191FFD0963774D2307C7510                   0                 0   \n",
       "\n",
       "                                  TimeWeekDaylight_EPI  TimeWeekNight_EPI  \\\n",
       "000004F96010462FA7A71C7784E27476                     0                  0   \n",
       "000007EEE7895E17937CFF86379C8E17                   144                  0   \n",
       "00000837B868F926F67BD440A0845C1D                     0                  0   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E                     0                  0   \n",
       "0000129F4191FFD0963774D2307C7510                     0                  0   \n",
       "\n",
       "                                  TimeWeekend_EPI  TOTAL_USERS  EPI_USERS  \\\n",
       "000004F96010462FA7A71C7784E27476                0            1          0   \n",
       "000007EEE7895E17937CFF86379C8E17                0            1          1   \n",
       "00000837B868F926F67BD440A0845C1D                0            3          0   \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E                0            1          0   \n",
       "0000129F4191FFD0963774D2307C7510                0            2          0   \n",
       "\n",
       "                                  EXP_USERS  y  \n",
       "000004F96010462FA7A71C7784E27476          1  0  \n",
       "000007EEE7895E17937CFF86379C8E17          1  0  \n",
       "00000837B868F926F67BD440A0845C1D          2  1  \n",
       "00000C5D3BFBEBD919F6F68386C3DD5E          0  0  \n",
       "0000129F4191FFD0963774D2307C7510          2  0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and var in input table\n",
    "Se sabe que las columnas con  mucha correlacion son malas para los modelos lineales (no tanto asi con RF o XGB ). A estas columnas seria bueno cambiarlas, editarlas/modificarlas para no seguir usando esta info.\n",
    "\n",
    "Las columnas de \"poca\" varianza no aportan informacion basicamente y por eso se dropean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 s, sys: 1.25 s, total: 22.7 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corr_series = X_train.corr().abs()\n",
    "#lo paso a una serie\n",
    "corr_series = corr_series.unstack()\n",
    "#ordeno por los valores mas altos\n",
    "corr_series = corr_series.sort_values(ascending = False) \n",
    "#es obvio que la diagonal tenia correlacion ==1, asi que elimino aquellos valores que eran de la diagonal\n",
    "#tambien saco los valores que estan \"arriba\" de la diagonal, pero haciendo la comparacion entre strings\n",
    "corr_series = corr_series[(corr_series.index.get_level_values(0) != corr_series.index.get_level_values(1))\\\n",
    "                         & (corr_series.index.get_level_values(0) <= corr_series.index.get_level_values(1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EXP_USERS              TOTAL_USERS             0.984376\n",
       "COUNT_8                COUNT_9                 0.918492\n",
       "COUNT_7                COUNT_8                 0.908408\n",
       "COUNT_6                COUNT_7                 0.895939\n",
       "COUNT_5                COUNT_6                 0.872578\n",
       "CallsWeekDaylight      TimeWeekDaylight        0.857456\n",
       "CallsWeekDaylight_EPI  TimeWeekDaylight_EPI    0.853580\n",
       "EPI_USERS              EXP_USERS               0.851134\n",
       "COUNT_7                COUNT_9                 0.834363\n",
       "EPI_USERS              TOTAL_USERS             0.828276\n",
       "CallsWeekNight         TimeWeekNight           0.825796\n",
       "CallsWeekNight_EPI     TimeWeekNight_EPI       0.823400\n",
       "COUNT_4                COUNT_5                 0.815779\n",
       "COUNT_6                COUNT_8                 0.813874\n",
       "CallsWeekend_EPI       TimeWeekend_EPI         0.794761\n",
       "CallsWeekend           TimeWeekend             0.794346\n",
       "COUNT_3                COUNT_4                 0.793586\n",
       "COUNT_5                COUNT_7                 0.781769\n",
       "CallsWeekNight         CallsWeekend            0.765076\n",
       "CallsWeekNight_EPI     CallsWeekend_EPI        0.764464\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_series.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# como resultado de esto decidimos cambiar las columnas\n",
    "# [ exp_USERS ] y repetir el algo\n",
    "# la idea es tomar la diferencia entre las dos columnas correlacionados pues puede aportar nueva info.\n",
    "X_train['EXP_USERS'] = X_train['TOTAL_USERS'] - X_train['EXP_USERS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.5 s, sys: 6.67 s, total: 48.1 s\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# miramos ahora las columnas con poca varianza ie. que aportan poca info\n",
    "\n",
    "var_matrix = X_train.var()\n",
    "#imprimimos aquellas que esten por abajo de cierto percentil\n",
    "perc = 0.5\n",
    "var_matrix[var_matrix<perc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# como resultado de esto decidimos dropear las columnas, por poca varianza..\n",
    "# [ EXPOSED ]\n",
    "X_train.drop('EXPOSED',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/miniconda2/lib/python2.7/site-packages/pandas/core/generic.py:2569: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  locs = rs.choice(axis_length, size=n, replace=replace, p=weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 s, sys: 7.15 s, total: 26.1 s\n",
      "Wall time: 9.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#con decomposicion qr tratamos de ver si existe alguna columna que sea comb linear de las demas\n",
    "linear_test = np.linalg.qr(X_train.sample(1e6))[1]\n",
    "#notar que devuelve la tabla de tamanyo N`columnas x Ncolumnas\n",
    "\n",
    "#sumo a traves de las columnas para que me de el valor absoluto sumado de c/fila\n",
    "linear_test = abs(linear_test.sum(axis=1))<1e-2\n",
    "#si hubiese alguan que sea linearcomb entonces tendria que aparecer que toda la fila es de ceros\n",
    "for i, col in enumerate(X_train.columns):\n",
    "    if linear_test[i] == True:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Simple Classification Tests\n",
    "La idea es probar tres clasificadores basicos y tener una medida de precision, recall, F1-score p/c/ estimador:\n",
    "\n",
    "* **Clasificador 1**: con la proporción de gente que vive en la zona endémica en el \"presente\", asignar la gente al azar respetando esa proporción i.e. $X \\sim Bernoulli(\\hat{p}) s.t. p$ is the mean of endemic users in the dataset\n",
    "\n",
    "* **Clasificador 2**: usando la información del \"presente\", si la persona vive en la zona endémica en el presente, predecir que va a ocurrir lo mismo en el \"pasado\" i.e. un estimador que devuelve el feature de endemicidad actual para predecir la endemicidad en el pasado.\n",
    "\n",
    "\n",
    "* **Clasificador 3**: usar 2 features del \"presente\", (i) si la persona vive en la zona endémica, (ii) cual es la proporción de sus contactos que viven en la zona endémica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clf 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/mobility-study/scikit-learn/sklearn/externals/joblib/_multiprocessing_helpers.py:29: UserWarning: [Errno 38] Function not implemented.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_hat = X_train['EPIDEMIC'].mean()\n",
    "#the output vector, i.e. the estimated epidemic users\n",
    "y_hat = stats.bernoulli.rvs(p_hat, size=len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A priori probability of bein epidemic is 0.293362195887\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.71      0.71   1413371\n",
      "          1       0.29      0.29      0.29    586914\n",
      "\n",
      "avg / total       0.58      0.58      0.58   2000285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"A priori probability of bein epidemic is %s\" %p_hat)\n",
    "print(classification_report(y_target, y_hat ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clf 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.71      0.71   1413371\n",
      "          1       0.29      0.29      0.29    586914\n",
      "\n",
      "avg / total       0.59      0.59      0.59   2000285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat = X_train['EPIDEMIC'].values\n",
    "print(classification_report(y_target, y_hat ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clf 3\n",
    "adf\n",
    "\n",
    "adsf\n",
    "\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('number of users who are and have been epidemic is 1171065',\n",
       " 'dataset size is 2000285')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'number of users who are and have been epidemic is %s'% (X_train['EPIDEMIC'].values == y_target).sum(), \"dataset size is %s\" % len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross, test, train splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.46 s, sys: 544 ms, total: 2.01 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#parto efectivamente el dataset en 3 conjunots, uno para fitear todo. otro para entrenar+transform y el ultimo\n",
    "#para transform +val\n",
    "msk = np.random.rand(len(X_train)) < 0.3333\n",
    "X_fit = X_train[msk]\n",
    "X_train =  X_train[~msk]\n",
    "msk2 = np.random.rand(len(X_train)) < 0.5\n",
    "X_val =  X_train[msk2]\n",
    "X_train =  X_train[~msk2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Featurizer on Antennas\n",
    "nos quedamos con las columnas de antennas y en graphlab aplicamos el algo de CountFeaturizer para cada categoria de\n",
    "Antenna_ID_N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A newer version of GraphLab Create (v1.10.1) is available! Your current version is v1.8.5.\n",
      "\n",
      "You can use pip to upgrade the graphlab-create package. For more information see https://dato.com/products/create/upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create is assigned to svega@dm.uba.ar and will expire on April 16, 2017. For commercial licensing options, visit https://dato.com/buy/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-06-14 00:17:59,281 [INFO] graphlab.cython.cy_server, 176: GraphLab Create v1.8.5 started. Logging: /tmp/graphlab_server_1465863477.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.21 s, sys: 1.51 s, total: 8.72 s\n",
      "Wall time: 7.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Count Featurizer on Antennas\n",
    "#nos quedamos con las columnas de antennas y pasamos todo a Graph Lab.\n",
    "import graphlab as gl\n",
    "from graphlab.toolkits.feature_engineering import *\n",
    "ant_cols = [col for col in X_train.columns if \"ANTENNA_ID\" in col]\n",
    "ant_sframe_fit = gl.SFrame(X_fit[ant_cols + ['y']])\n",
    "ant_sframe_train = gl.SFrame(X_train[ant_cols + ['y']])\n",
    "ant_sframe_val = gl.SFrame(X_val[ant_cols + ['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.6 s, sys: 8.91 s, total: 1min 7s\n",
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "countfeat = gl.feature_engineering.create(ant_sframe_fit, \n",
    "               CountFeaturizer(target='y'))\n",
    "\n",
    "# Transform the train set. This is the dataset I will train my classifier on\n",
    "transformed_ant_train = countfeat.transform(ant_sframe_train)\n",
    "transformed_ant_val = countfeat.transform(ant_sframe_val)\n",
    "\n",
    "del ant_sframe_fit,ant_sframe_train,ant_sframe_val\n",
    "\n",
    "#por alguna razon guarda los valores de probabilidad como una lista de un unico valor\n",
    "for col in [col for col in transformed_ant_train.column_names() if \"prob_\" in col]:\n",
    "    transformed_ant_train[col] = transformed_ant_train[col].apply(lambda x: x[0]) \n",
    "    transformed_ant_val[col] = transformed_ant_val[col].apply(lambda x: x[0]) \n",
    "\n",
    "#me quedo solo con los valores de probabilidad.\n",
    "transformed_ant_train = transformed_ant_train[[col for col in transformed_ant_train.column_names() if \"prob_\" in col]]\n",
    "transformed_ant_val = transformed_ant_val[[col for col in transformed_ant_train.column_names() if \"prob_\" in col]]\n",
    "\n",
    "transformed_ant_val = transformed_ant_val.to_dataframe()\n",
    "transformed_ant_train = transformed_ant_train.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val[ant_cols] = transformed_ant_val.values\n",
    "X_train[ant_cols] = transformed_ant_train.values\n",
    "del transformed_ant_train, transformed_ant_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit estimator fit and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/mobility-study/scikit-learn/sklearn/cross_validation.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/juan/mobility-study/scikit-learn/sklearn/grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "#from sklearn.linear_model import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.utils import *\n",
    "from sklearn.preprocessing import label_binarize, scale, StandardScaler\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.grid_search import *\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new variables\n",
    "These would be more \"real\" variables i.e. variables that would be \"humanly\" explainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ANTENNA_ID_0', u'COUNT_0', u'ANTENNA_ID_1', u'COUNT_1',\n",
       "       u'ANTENNA_ID_2', u'COUNT_2', u'ANTENNA_ID_3', u'COUNT_3',\n",
       "       u'ANTENNA_ID_4', u'COUNT_4', u'ANTENNA_ID_5', u'COUNT_5',\n",
       "       u'ANTENNA_ID_6', u'COUNT_6', u'ANTENNA_ID_7', u'COUNT_7',\n",
       "       u'ANTENNA_ID_8', u'COUNT_8', u'ANTENNA_ID_9', u'COUNT_9',\n",
       "       u'MOBILITY_DIAMETER', u'EPIDEMIC', u'CallsWeekDaylight',\n",
       "       u'CallsWeekNight', u'CallsWeekend', u'TimeWeekDaylight',\n",
       "       u'TimeWeekNight', u'TimeWeekend', u'CallsWeekDaylight_EPI',\n",
       "       u'CallsWeekNight_EPI', u'CallsWeekend_EPI', u'TimeWeekDaylight_EPI',\n",
       "       u'TimeWeekNight_EPI', u'TimeWeekend_EPI', u'TOTAL_USERS', u'EPI_USERS',\n",
       "       u'EXP_USERS', u'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns\n",
    "#tenemos 38 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new features\n",
    "these would be \"synthetic\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_X_files(tables, scaled=False, poly2=False,train_size = 0.8):\n",
    "    #como input tiene que entrar la tabla sin splitea en train_test aun\n",
    "    #tiene que suceder que haya consistencia entre las columnas de c/dataframe tambien\n",
    "    \n",
    "    \n",
    "    calls_cols = [col for col in X_train.columns if \"Calls\" in col] \n",
    "    time_cols = [col for col in X_train.columns if \"Time\" in col]\n",
    "    epi_calls_cols = [col for col in X_train.columns if \"Calls\" in col and \"EPI\" in col] \n",
    "    epi_time_cols = [col for col in X_train.columns if \"Time\" in col and \"EPI\" in col]\n",
    "    antenna_cols = [col for col in X_train.columns if \"ANTENNA_\" in col]\n",
    "    count_cols = [col for col in X_train.columns if \"COUNT_\" in col]\n",
    "    condition_cols = [col for col in X_train.columns if \"EXPOSED\" == col or \"EPIDEMIC\" == col]\n",
    "    \n",
    "    categorical_cols = condition_cols\n",
    "    numerical_cols = [col for col in table.columns if col not in categorical_cols]\n",
    "\n",
    "    print(\"First dataframe is %s big\" % str(train_table.shape))\n",
    "    #print(\"Test dataframe is %s big\" % str(test_table.shape))\n",
    "    \n",
    "    X_train_categorical = train_table[categorical_cols].values\n",
    "    X_train_numeric = train_table[numeric_cols].values\n",
    "    \n",
    "    if scaled ==True :\n",
    "            min_max_scaler = MinMaxScaler().fit(X_train_count_time)\n",
    "            X_train_count_time = min_max_scaler.transform(X_train_numeric)\n",
    "    \n",
    "    #hacemos interacciones polinomiales sobre las columnas de count/time\n",
    "    if poly2 == True:\n",
    "        #pensar que hacer polynomial features es masomenos como agregar nˆ2 nuevas columnas\n",
    "        #con lo cual tenemos que tener cuidado en no reventar la memoria, ponemor las primeras 50 columnas \n",
    "        #como tope para aplicar interacciones polinomiales\n",
    "\n",
    "        poly2_transform = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "        max_cols = 50\n",
    "\n",
    "        if  (X_train_count_time.shape[1] <= max_cols):\n",
    "            max_cols = X_train_count_time.shape[1]\n",
    "        poly2_transform.fit(X_train_count_time[:,0:max_cols])\n",
    "\n",
    "        X_train_poly2 = poly2_transform.transform(X_train_count_time[:,0:max_cols])\n",
    "        X_train_count_time = np.concatenate( ( X_train_poly2, X_train_count_time[:,max_cols:]),axis=1) \n",
    "\n",
    "    X_train = np.concatenate( ( X_train_count_time, X_train_categorical),axis=1)\n",
    "    \n",
    "    processed_tables = [X_train]\n",
    "    \n",
    "    #aca basicamente replicamos lo anterior pero para todo el resto de las tablas y utilizando los fits que ya tenemos\n",
    "\n",
    "    for i in range(len(tables)):\n",
    "        # skip the X_train table which comes in the first postion and will behave as our special fitting table\n",
    "        if i ==0:\n",
    "            continue\n",
    "        print(i)\n",
    "        X_val_categorical = tables[i][categorical_cols].values\n",
    "        X_val_count_time = tables[i][count_time_cols].values\n",
    "\n",
    "        if scaled ==True :\n",
    "            X_val_count_time = min_max_scaler.transform(X_val_count_time)\n",
    "\n",
    "        #hacemos interacciones polinomiales sobre las columnas de count/time\n",
    "        if poly2 == True:\n",
    "            #pensar que hacer polynomial features es masomenos como agregar nˆ2 nuevas columnas\n",
    "            #con lo cual tenemos que tener cuidado en no reventar la memoria, ponemor las primeras 50 columnas \n",
    "            #como tope para aplicar interacciones polinomiales\n",
    "\n",
    "            X_val_poly2 = poly2_transform.transform(X_val_count_time[:,0:max_cols])\n",
    "            X_val_count_time = np.concatenate( ( X_val_poly2, X_val_count_time[:,max_cols:]),axis=1) \n",
    "\n",
    "        X_val = np.concatenate( ( X_val_count_time, X_val_categorical),axis=1)\n",
    "        \n",
    "        processed_tables = processed_tables + [X_val]\n",
    "    \n",
    "    for i in range(len(processed_tables)):    \n",
    "        print(\"Table {0} shape is {1}\".format( i,str(processed_tables[i].shape)))\n",
    "#        print(\"Table %s categorical shape is %s\" % str(X_val_categorical.shape))\n",
    "#        print(\"Val non-categorical shape is %s\" % str(X_val_count_time.shape))\n",
    "#        print(\"Val categorical shape is %s\" % str(X_val_categorical.shape))\n",
    "    \n",
    "    return processed_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-d11348c8b532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## seteo los generalizadores de CV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m##genero los estimadores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#model = GradientBoostingClassifier()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "## seteo los generalizadores de CV\n",
    "##genero los estimadores\n",
    "W = np.array([15 if i == 1 else 1 for i in y])\n",
    "#model = GradientBoostingClassifier()\n",
    "\n",
    "gradboost = GradientBoostingClassifier( loss = 'deviance',n_estimators=20, \n",
    "                                       max_depth = 20\n",
    "                                 )\n",
    "svc = LinearSVC(C=2.0, tol = 1e-4,\n",
    "                class_weight = 'balanced'\n",
    "               )\n",
    "\n",
    "svc2 = SVC(C=2.0, kernel = 'rbf' ,decision_function_shape='ovr',tol = 1e-5,\n",
    "                class_weight = 'balanced', max_iter = 1000\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes - Benchmark - Cross Validation\n",
    "mnb es bastante rapido y eficiente para rapidamente resultados en problemas de clasificacion supervisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] alpha=0, fit_prior=True .........................................\n",
      "[CV] ................ alpha=0, fit_prior=True, score=0.297282 -   2.9s\n",
      "[CV] alpha=0, fit_prior=True .........................................\n",
      "[CV] ................ alpha=0, fit_prior=True, score=0.294858 -   2.9s\n",
      "[CV] alpha=0, fit_prior=True .........................................\n",
      "[CV] ................ alpha=0, fit_prior=True, score=0.295935 -   2.9s\n",
      "[CV] alpha=0, fit_prior=False ........................................\n",
      "[CV] ............... alpha=0, fit_prior=False, score=0.297254 -   2.9s\n",
      "[CV] alpha=0, fit_prior=False ........................................\n",
      "[CV] ............... alpha=0, fit_prior=False, score=0.294880 -   2.9s\n",
      "[CV] alpha=0, fit_prior=False ........................................\n",
      "[CV] ............... alpha=0, fit_prior=False, score=0.295969 -   3.0s\n",
      "[CV] alpha=0.5, fit_prior=True .......................................\n",
      "[CV] .............. alpha=0.5, fit_prior=True, score=0.297282 -   2.9s\n",
      "[CV] alpha=0.5, fit_prior=True .......................................\n",
      "[CV] .............. alpha=0.5, fit_prior=True, score=0.294858 -   2.9s\n",
      "[CV] alpha=0.5, fit_prior=True .......................................\n",
      "[CV] .............. alpha=0.5, fit_prior=True, score=0.295935 -   2.9s\n",
      "[CV] alpha=0.5, fit_prior=False ......................................\n",
      "[CV] ............. alpha=0.5, fit_prior=False, score=0.297254 -   2.9s\n",
      "[CV] alpha=0.5, fit_prior=False ......................................\n",
      "[CV] ............. alpha=0.5, fit_prior=False, score=0.294880 -   2.9s\n",
      "[CV] alpha=0.5, fit_prior=False ......................................\n",
      "[CV] ............. alpha=0.5, fit_prior=False, score=0.295969 -   2.9s\n",
      "[CV] alpha=1.0, fit_prior=True .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:   45.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. alpha=1.0, fit_prior=True, score=0.297282 -   2.9s\n",
      "[CV] alpha=1.0, fit_prior=True .......................................\n",
      "[CV] .............. alpha=1.0, fit_prior=True, score=0.294858 -   2.9s\n",
      "[CV] alpha=1.0, fit_prior=True .......................................\n",
      "[CV] .............. alpha=1.0, fit_prior=True, score=0.295935 -   2.9s\n",
      "[CV] alpha=1.0, fit_prior=False ......................................\n",
      "[CV] ............. alpha=1.0, fit_prior=False, score=0.297254 -   3.0s\n",
      "[CV] alpha=1.0, fit_prior=False ......................................\n",
      "[CV] ............. alpha=1.0, fit_prior=False, score=0.294880 -   2.9s\n",
      "[CV] alpha=1.0, fit_prior=False ......................................\n",
      "[CV] ............. alpha=1.0, fit_prior=False, score=0.295969 -   3.0s\n",
      "[CV] alpha=0.1, fit_prior=True .......................................\n",
      "[CV] .............. alpha=0.1, fit_prior=True, score=0.297282 -   3.0s\n",
      "[CV] alpha=0.1, fit_prior=True .......................................\n",
      "[CV] .............. alpha=0.1, fit_prior=True, score=0.294858 -   3.0s\n",
      "[CV] alpha=0.1, fit_prior=True .......................................\n",
      "[CV] alpha=0.1, fit_prior=False ......................................\n",
      "[CV] .............. alpha=0.1, fit_prior=True, score=0.295935 -   5.0s\n",
      "[CV] alpha=0.1, fit_prior=False ......................................\n",
      "[CV] ............. alpha=0.1, fit_prior=False, score=0.297254 -   5.1s\n",
      "[CV] alpha=0.1, fit_prior=False ......................................\n",
      "[CV] ............. alpha=0.1, fit_prior=False, score=0.294880 -   5.0s\n",
      "[CV] alpha=0.01, fit_prior=True ......................................\n",
      "[CV] ............. alpha=0.1, fit_prior=False, score=0.295969 -   5.0s\n",
      "[CV] alpha=0.01, fit_prior=True ......................................\n",
      "[CV] ............. alpha=0.01, fit_prior=True, score=0.297282 -   5.0s\n",
      "[CV] alpha=0.01, fit_prior=True ......................................\n",
      "[CV] ............. alpha=0.01, fit_prior=True, score=0.294858 -   5.0s\n",
      "[CV] alpha=0.01, fit_prior=False .....................................\n",
      "[CV] ............. alpha=0.01, fit_prior=True, score=0.295935 -   5.1s\n",
      "[CV] alpha=0.01, fit_prior=False .....................................\n",
      "[CV] ............ alpha=0.01, fit_prior=False, score=0.297254 -   5.0s\n",
      "[CV] alpha=0.01, fit_prior=False .....................................\n",
      "[CV] ............ alpha=0.01, fit_prior=False, score=0.294880 -   5.0s\n",
      "[CV] alpha=10.0, fit_prior=True ......................................\n",
      "[CV] ............ alpha=0.01, fit_prior=False, score=0.295969 -   5.1s\n",
      "[CV] alpha=10.0, fit_prior=True ......................................\n",
      "[CV] ............. alpha=10.0, fit_prior=True, score=0.297282 -   5.0s\n",
      "[CV] alpha=10.0, fit_prior=True ......................................\n",
      "[CV] ............. alpha=10.0, fit_prior=True, score=0.294858 -   5.0s\n",
      "[CV] alpha=10.0, fit_prior=False .....................................\n",
      "[CV] ............. alpha=10.0, fit_prior=True, score=0.295935 -   5.0s\n",
      "[CV] alpha=10.0, fit_prior=False .....................................\n",
      "[CV] ............ alpha=10.0, fit_prior=False, score=0.297254 -   5.0s\n",
      "[CV] alpha=10.0, fit_prior=False .....................................\n",
      "[CV] ............ alpha=10.0, fit_prior=False, score=0.294880 -   5.0s\n",
      "[CV] ............ alpha=10.0, fit_prior=False, score=0.295969 -   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  36 out of  36 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search took 138.141034126 seconds to run\n",
      "Grid Search took 0.000313997268677 seconds to run\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.72      0.71     47103\n",
      "          1       0.29      0.27      0.28     19520\n",
      "\n",
      "avg / total       0.58      0.59      0.59     66623\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.72      0.71     93945\n",
      "          1       0.30      0.28      0.29     39301\n",
      "\n",
      "avg / total       0.59      0.59      0.59    133246\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.72      0.71    141314\n",
      "          1       0.29      0.28      0.28     58555\n",
      "\n",
      "avg / total       0.59      0.59      0.59    199869\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      1.00      0.83    188401\n",
      "          1       0.30      0.00      0.01     78091\n",
      "\n",
      "avg / total       0.59      0.71      0.59    266492\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.83      0.76    470763\n",
      "          1       0.29      0.17      0.21    195467\n",
      "\n",
      "avg / total       0.59      0.64      0.60    666230\n",
      "\n",
      "* Rates for distinct percentage groups \n",
      "\n",
      "10 %% group rate: 0.292991909701 size: 66623\n",
      "20 %% group rate: 0.294950692704 size: 133246\n",
      "30 %% group rate: 0.292966893315 size: 199869\n",
      "40 %% group rate: 0.293033186737 size: 266492\n",
      "This cell took 147.492668152 seconds to run\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "param_grid = {'alpha':[0,1.0, 1e-1,1e1], 'fit_prior': [True,False],\n",
    "             }\n",
    "estimator  = MultinomialNB( )\n",
    "\n",
    "clf =GridSearchCV(estimator, param_grid, scoring='roc_auc', fit_params=None, n_jobs=10, iid=True, refit=True, \n",
    "verbose=3, pre_dispatch='2*n_jobs', error_score='raise')\n",
    "n_iter_search =60\n",
    "\n",
    "#how many parameters to randomly search for\n",
    "n_iter_search = 45\n",
    "\n",
    "#clf = RandomizedSearchCV(estimator, param_distributions=param_grid,\n",
    "#                                   n_iter=n_iter_search, n_jobs =-1, verbose=3)\n",
    "X = X_train[X_train.columns[:-1]].values\n",
    "Y = X_train['y'].values\n",
    "\n",
    "#X = X_train_categorical\n",
    "clf.fit(X,Y)\n",
    "\n",
    "\n",
    "#random_search.fit(X_train,y_train)\n",
    "elapsed_time =  time.time() - start_time\n",
    "\n",
    "print('Random Search took %s seconds to run' % elapsed_time)\n",
    "\n",
    "\n",
    "all_time =   time.time() - start_time \n",
    "print('Grid Search took %s seconds to run' % (all_time - elapsed_time))\n",
    "\n",
    "\n",
    "X_test = X_val[X_train.columns[:-1]].values\n",
    "Y_test = X_val['y'].values\n",
    "\n",
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_test).max(axis=1)\n",
    "predicted_proba.index = X_val.index\n",
    "predicted_proba['class'] = clf.predict(X_test)\n",
    "predicted_proba['gt'] =  Y_test\n",
    "predicted_proba.index.name = \"USER\"\n",
    "predicted_proba.sort_values(by = 'prob',ascending = False, inplace = True)\n",
    "\n",
    "cut_percentages = [10,20,30,40]\n",
    "#converted = (predicted_proba['class']==1)\n",
    "#me fijo todos los users que van a convertir en el futuro\n",
    "\n",
    "past_epidemic = X_val[X_val['y']==1].index.values\n",
    "\n",
    "start = 0\n",
    "mobility_dict = {}\n",
    "index_segments = []\n",
    "for i in cut_percentages:\n",
    "    cut_size = int(len(predicted_proba)*i/100.0)    \n",
    "    bool_array = (~np.ones(len(predicted_proba), dtype=bool))\n",
    "    bool_array[start: start + cut_size] = True\n",
    "    index_segments += [bool_array]\n",
    "    start = start + cut_size\n",
    "    #subtable = \n",
    "    mobility_dict[str(i)] = predicted_proba.loc[index_segments[-1]].index.values\n",
    "    \n",
    "    print(classification_report(predicted_proba.loc[index_segments[-1],\"gt\"].values,\n",
    "                            predicted_proba.loc[index_segments[-1],'class'].values ))\n",
    "    \n",
    "    \n",
    "print(classification_report(predicted_proba.loc[:,\"gt\"].values,\n",
    "                            predicted_proba.loc[:,'class'].values ))\n",
    "    \n",
    "    \n",
    "print(\"* Rates for distinct percentage groups \\n\")\n",
    "for i in cut_percentages:\n",
    "    subtable = mobility_dict[str(i)]\n",
    "\n",
    "    print(\"{0} %% group rate: {1} size: {2}\".format(str(i), \n",
    "                                np.in1d(subtable,past_epidemic).sum()*1.0/len(subtable),len(subtable))) \n",
    "    \n",
    "    \n",
    "cv_result = pd.DataFrame(columns=['mean_score' ] + [key for key, value in clf.grid_scores_[0].parameters.items() ] )\n",
    "i=0\n",
    "for setup in clf.grid_scores_:\n",
    "    row = [clf.grid_scores_[i].mean_validation_score ] + [value for key, value in clf.grid_scores_[i].parameters.items() ]\n",
    "    cv_result.loc[i] = row\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "    \n",
    "all_time =   time.time() - start_time\n",
    "\n",
    "#este flag es basicamente para chequear que hayamos terminado nomas\n",
    "finished = True\n",
    "\n",
    "print('This cell took %s seconds to run' % all_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha\n",
      "0.00     0.500891\n",
      "0.01     0.500891\n",
      "0.10     0.500891\n",
      "0.50     0.500891\n",
      "1.00     0.500891\n",
      "10.00    0.500911\n",
      "Name: mean_score, dtype: float64\n",
      "\n",
      " std_deviation is\n",
      "alpha\n",
      "0.00     0.000066\n",
      "0.01     0.000066\n",
      "0.10     0.000066\n",
      "0.50     0.000066\n",
      "1.00     0.000066\n",
      "10.00    0.000088\n",
      "Name: mean_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coln = 1\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].mean())\n",
    "print('\\n std_deviation is')\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "* fitting prior has better mean and almost same std\n",
    "* alpha should be only > 0. But difference between positive alphas is negligible, even on different exponential orders\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.85      0.75     43545\n",
      "          1       0.40      0.19      0.25     22464\n",
      "\n",
      "avg / total       0.58      0.63      0.58     66009\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.75      0.69     83353\n",
      "          1       0.40      0.28      0.33     48666\n",
      "\n",
      "avg / total       0.55      0.58      0.56    132019\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.67      0.65    124185\n",
      "          1       0.39      0.35      0.37     73843\n",
      "\n",
      "avg / total       0.54      0.55      0.55    198028\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.56      0.59    164393\n",
      "          1       0.38      0.44      0.41     99645\n",
      "\n",
      "avg / total       0.53      0.52      0.52    264038\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.66      0.65    415477\n",
      "          1       0.39      0.36      0.37    244618\n",
      "\n",
      "avg / total       0.54      0.55      0.55    660095\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegressionCV' object has no attribute 'grid_scores_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-53e73f014a4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mcv_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_score'\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msetup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegressionCV' object has no attribute 'grid_scores_'"
     ]
    }
   ],
   "source": [
    "\n",
    "clf =  LogisticRegressionCV(n_jobs =3, class_weight= 'balanced', scoring = 'f1',cv=3)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_test).max(axis=1)\n",
    "predicted_proba['class'] = clf.predict(X_test)\n",
    "predicted_proba[gt] =  y_test\n",
    "predicted_proba.sort_values(by = 'prob',ascending = False, inplace = True)\n",
    "\n",
    "cut_percentages = [10,20,30,40]\n",
    "#converted = (predicted_proba['class']==1)\n",
    "#me fijo todos los users que van a convertir en el futuro\n",
    "\n",
    "\n",
    "start = 0\n",
    "mobility_dict = {}\n",
    "index_segments = []\n",
    "for i in cut_percentages:\n",
    "    cut_size = int(len(predicted_proba)*i/100.0)    \n",
    "    bool_array = (~np.ones(len(predicted_proba), dtype=bool))\n",
    "    bool_array[start: start + cut_size] = True\n",
    "    index_segments += [bool_array]\n",
    "    start = start + cut_size\n",
    "#    subtable = \n",
    "    #mobility_dict[str(i)] = predicted_proba.loc[index_segments[-1],'ref_hash']\n",
    "    \n",
    "    print(classification_report(predicted_proba.loc[index_segments[-1],gt].values,\n",
    "                            predicted_proba.loc[index_segments[-1],'class'].values ))\n",
    "    \n",
    "    \n",
    "print(classification_report(predicted_proba.loc[:,gt].values,\n",
    "                            predicted_proba.loc[:,'class'].values ))\n",
    "    \n",
    "    \n",
    "cv_result = pd.DataFrame(columns=['mean_score' ] + [key for key, value in clf.grid_scores_[0].parameters.items() ] )\n",
    "i=0\n",
    "for setup in clf.grid_scores_:\n",
    "    row = [clf.grid_scores_[i].mean_validation_score ] + [value for key, value in clf.grid_scores_[i].parameters.items() ]\n",
    "    cv_result.loc[i] = row\n",
    "    i+=1\n",
    "#este flag es basicamente para chequear que hayamos terminado nomas\n",
    "finished = True\n",
    "\n",
    "all_time =   time.time() - start_time\n",
    "\n",
    "#este flag es basicamente para chequear que hayamos terminado nomas\n",
    "finished = True\n",
    "\n",
    "print('This cell took %s seconds to run' % all_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  { 'alpha': [1e-3,5*1e-3,1e-2], 'l1_ratio': [3*1e-2,1e-3,6*1e-4], 'n_iter': [2,3]\n",
    "  }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_iter=2, alpha=0.001, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_iter=2, alpha=0.001, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=2, alpha=0.001, l1_ratio=0.03, score=0.125354 -   2.5s\n",
      "[CV] n_iter=2, alpha=0.001, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=2, alpha=0.001, l1_ratio=0.03, score=0.127442 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.001, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=2, alpha=0.001, l1_ratio=0.03, score=0.119463 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.001, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=3, alpha=0.001, l1_ratio=0.03, score=0.132910 -   3.1s\n",
      "[CV] n_iter=3, alpha=0.001, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=3, alpha=0.001, l1_ratio=0.03, score=0.133858 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.001, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=3, alpha=0.001, l1_ratio=0.03, score=0.125862 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.001, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=2, alpha=0.001, l1_ratio=0.001, score=0.114065 -   2.5s\n",
      "[CV] n_iter=2, alpha=0.001, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=2, alpha=0.001, l1_ratio=0.001, score=0.125576 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.001, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=2, alpha=0.001, l1_ratio=0.001, score=0.125882 -   2.8s\n",
      "[CV] n_iter=3, alpha=0.001, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=3, alpha=0.001, l1_ratio=0.001, score=0.138072 -   3.3s\n",
      "[CV] n_iter=3, alpha=0.001, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=3, alpha=0.001, l1_ratio=0.001, score=0.133482 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.001, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=3, alpha=0.001, l1_ratio=0.001, score=0.133945 -   3.2s\n",
      "[CV] n_iter=2, alpha=0.001, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=2, alpha=0.001, l1_ratio=0.0006000000000000001, score=0.124318 -   2.5s\n",
      "[CV] n_iter=2, alpha=0.001, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=2, alpha=0.001, l1_ratio=0.0006000000000000001, score=0.125224 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.001, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=2, alpha=0.001, l1_ratio=0.0006000000000000001, score=0.129423 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.001, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=3, alpha=0.001, l1_ratio=0.0006000000000000001, score=0.131845 -   3.1s\n",
      "[CV] n_iter=3, alpha=0.001, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=3, alpha=0.001, l1_ratio=0.0006000000000000001, score=0.135827 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.005, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=3, alpha=0.001, l1_ratio=0.0006000000000000001, score=0.128485 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.005, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=2, alpha=0.005, l1_ratio=0.03, score=0.125477 -   2.4s\n",
      "[CV] n_iter=2, alpha=0.005, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=2, alpha=0.005, l1_ratio=0.03, score=0.125957 -   2.8s\n",
      "[CV] n_iter=3, alpha=0.005, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=2, alpha=0.005, l1_ratio=0.03, score=0.127511 -   2.6s\n",
      "[CV] n_iter=3, alpha=0.005, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=3, alpha=0.005, l1_ratio=0.03, score=0.127618 -   2.8s\n",
      "[CV] n_iter=3, alpha=0.005, l1_ratio=0.03 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=3, alpha=0.005, l1_ratio=0.03, score=0.125386 -   2.8s\n",
      "[CV] n_iter=2, alpha=0.005, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=3, alpha=0.005, l1_ratio=0.03, score=0.136983 -   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   47.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_iter=2, alpha=0.005, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=2, alpha=0.005, l1_ratio=0.001, score=0.120290 -   2.5s\n",
      "[CV] n_iter=2, alpha=0.005, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=2, alpha=0.005, l1_ratio=0.001, score=0.118753 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.005, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=2, alpha=0.005, l1_ratio=0.001, score=0.127159 -   2.9s\n",
      "[CV] n_iter=3, alpha=0.005, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=3, alpha=0.005, l1_ratio=0.001, score=0.127485 -   3.3s\n",
      "[CV] n_iter=3, alpha=0.005, l1_ratio=0.001 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=3, alpha=0.005, l1_ratio=0.001, score=0.139728 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.005, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. n_iter=3, alpha=0.005, l1_ratio=0.001, score=0.130469 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.005, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=2, alpha=0.005, l1_ratio=0.0006000000000000001, score=0.129161 -   2.5s\n",
      "[CV] n_iter=2, alpha=0.005, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=2, alpha=0.005, l1_ratio=0.0006000000000000001, score=0.130159 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.005, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=2, alpha=0.005, l1_ratio=0.0006000000000000001, score=0.127613 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.005, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=3, alpha=0.005, l1_ratio=0.0006000000000000001, score=0.132956 -   3.1s\n",
      "[CV] n_iter=3, alpha=0.005, l1_ratio=0.0006000000000000001 ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=3, alpha=0.005, l1_ratio=0.0006000000000000001, score=0.143005 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.01, l1_ratio=0.03 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=3, alpha=0.005, l1_ratio=0.0006000000000000001, score=0.135926 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.01, l1_ratio=0.03 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... n_iter=2, alpha=0.01, l1_ratio=0.03, score=0.128098 -   2.4s\n",
      "[CV] n_iter=2, alpha=0.01, l1_ratio=0.03 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... n_iter=2, alpha=0.01, l1_ratio=0.03, score=0.127078 -   2.7s\n",
      "[CV] n_iter=3, alpha=0.01, l1_ratio=0.03 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... n_iter=2, alpha=0.01, l1_ratio=0.03, score=0.126921 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.01, l1_ratio=0.03 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... n_iter=3, alpha=0.01, l1_ratio=0.03, score=0.138152 -   3.6s\n",
      "[CV] n_iter=3, alpha=0.01, l1_ratio=0.03 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... n_iter=3, alpha=0.01, l1_ratio=0.03, score=0.106208 -   3.1s\n",
      "[CV] n_iter=2, alpha=0.01, l1_ratio=0.001 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... n_iter=3, alpha=0.01, l1_ratio=0.03, score=0.143076 -   2.8s\n",
      "[CV] n_iter=2, alpha=0.01, l1_ratio=0.001 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=2, alpha=0.01, l1_ratio=0.001, score=0.132427 -   2.5s\n",
      "[CV] n_iter=2, alpha=0.01, l1_ratio=0.001 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=2, alpha=0.01, l1_ratio=0.001, score=0.125901 -   2.9s\n",
      "[CV] n_iter=3, alpha=0.01, l1_ratio=0.001 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=2, alpha=0.01, l1_ratio=0.001, score=0.128001 -   2.7s\n",
      "[CV] n_iter=3, alpha=0.01, l1_ratio=0.001 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=3, alpha=0.01, l1_ratio=0.001, score=0.135568 -   3.1s\n",
      "[CV] n_iter=3, alpha=0.01, l1_ratio=0.001 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_iter=2, alpha=0.01, l1_ratio=0.0006000000000000001 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... n_iter=3, alpha=0.01, l1_ratio=0.001, score=0.132579 -   4.4s\n",
      "[CV] ... n_iter=3, alpha=0.01, l1_ratio=0.001, score=0.136589 -   3.7s\n",
      "[CV] n_iter=2, alpha=0.01, l1_ratio=0.0006000000000000001 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=2, alpha=0.01, l1_ratio=0.0006000000000000001, score=0.124134 -   2.8s\n",
      "[CV] n_iter=2, alpha=0.01, l1_ratio=0.0006000000000000001 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=2, alpha=0.01, l1_ratio=0.0006000000000000001, score=0.130883 -   2.5s\n",
      "[CV] n_iter=3, alpha=0.01, l1_ratio=0.0006000000000000001 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=2, alpha=0.01, l1_ratio=0.0006000000000000001, score=0.122741 -   3.5s\n",
      "[CV] n_iter=3, alpha=0.01, l1_ratio=0.0006000000000000001 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by\n",
      "descriptor assignment is deprecated. To maintain\n",
      "the Fortran contiguity of a multidimensional Fortran\n",
      "array, use 'a.T.view(...).T' instead\n",
      "  obj_bytes_view = obj.view(self.np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_iter=3, alpha=0.01, l1_ratio=0.0006000000000000001, score=0.132433 -   3.7s\n",
      "[CV] n_iter=3, alpha=0.01, l1_ratio=0.0006000000000000001 ............\n",
      "[CV]  n_iter=3, alpha=0.01, l1_ratio=0.0006000000000000001, score=0.135111 -   3.4s\n",
      "[CV]  n_iter=3, alpha=0.01, l1_ratio=0.0006000000000000001, score=0.135908 -   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  54 out of  54 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.31      0.47    355992\n",
      "          1       0.06      0.99      0.11     15152\n",
      "\n",
      "avg / total       0.96      0.33      0.45    371144\n",
      "\n",
      "This cell took 108.56432461738586 seconds to run\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "param_grid = [\n",
    "  { 'alpha': [1e-3,5*1e-3,1e-2], 'l1_ratio': [3*1e-2,1e-3,6*1e-4], 'n_iter': [2,3]\n",
    "  }\n",
    "    ]\n",
    "sgd = SGDClassifier(loss='modified_huber', penalty='elasticnet', \n",
    "             fit_intercept=True,  shuffle=True, \n",
    "                    n_jobs=3,learning_rate='optimal', power_t =2, eta0 =5,\n",
    "                    class_weight='balanced', average=40)\n",
    "\n",
    "clf =GridSearchCV(sgd, param_grid, scoring='roc_auc', fit_params=None, n_jobs=4, iid=True, refit=True, \n",
    "verbose=3, pre_dispatch='2*n_jobs', error_score='raise')\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_test).max(axis=1)\n",
    "predicted_proba['class'] = clf.predict(X_test)\n",
    "predicted_proba[gt] =  y_test\n",
    "predicted_proba.sort_values(by = 'prob',ascending = False, inplace = True)\n",
    "\n",
    "cut_percentages = [10,20,30,40]\n",
    "#converted = (predicted_proba['class']==1)\n",
    "#me fijo todos los users que van a convertir en el futuro\n",
    "\n",
    "\n",
    "start = 0\n",
    "mobility_dict = {}\n",
    "index_segments = []\n",
    "for i in cut_percentages:\n",
    "    cut_size = int(len(predicted_proba)*i/100.0)    \n",
    "    bool_array = (~np.ones(len(predicted_proba), dtype=bool))\n",
    "    bool_array[start: start + cut_size] = True\n",
    "    index_segments += [bool_array]\n",
    "    start = start + cut_size\n",
    "#    subtable = \n",
    "    #mobility_dict[str(i)] = predicted_proba.loc[index_segments[-1],'ref_hash']\n",
    "    \n",
    "    print(classification_report(predicted_proba.loc[index_segments[-1],gt].values,\n",
    "                            predicted_proba.loc[index_segments[-1],'class'].values ))\n",
    "    \n",
    "    \n",
    "print(classification_report(predicted_proba.loc[:,gt].values,\n",
    "                            predicted_proba.loc[:,'class'].values ))\n",
    "    \n",
    "    \n",
    "cv_result = pd.DataFrame(columns=['mean_score' ] + [key for key, value in clf.grid_scores_[0].parameters.items() ] )\n",
    "i=0\n",
    "for setup in clf.grid_scores_:\n",
    "    row = [clf.grid_scores_[i].mean_validation_score ] + [value for key, value in clf.grid_scores_[i].parameters.items() ]\n",
    "    cv_result.loc[i] = row\n",
    "    i+=1\n",
    "#este flag es basicamente para chequear que hayamos terminado nomas\n",
    "finished = True\n",
    "\n",
    "all_time =   time.time() - start_time\n",
    "\n",
    "#este flag es basicamente para chequear que hayamos terminado nomas\n",
    "finished = True\n",
    "\n",
    "print('This cell took %s seconds to run' % all_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "ind = predicted_proba['prob']>threshold\n",
    "print(classification_report(predicted_proba.loc[ind,'y'].values,predicted_proba.loc[ind,'class'].values ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.005, average=40, class_weight='balanced', epsilon=0.1,\n",
       "       eta0=5, fit_intercept=True, l1_ratio=0.0006000000000000001,\n",
       "       learning_rate='optimal', loss='modified_huber', n_iter=3, n_jobs=3,\n",
       "       penalty='elasticnet', power_t=2, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter\n",
      "2.0    0.125741\n",
      "3.0    0.132943\n",
      "Name: mean_score, dtype: float64\n",
      "n_iter\n",
      "2.0    0.002609\n",
      "3.0    0.002703\n",
      "Name: mean_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coln=1\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].mean())\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results\n",
    "* l1_ratio = cuanto mas chico mejor con lo cual la perdida l2 parece ser mejor\n",
    "* alpha = 1e-3 es suficiente pues casi no afecta el score\n",
    "* power_t = muy variado, no parece haber correlacion entre el tamanyo y el avg, mean_score\n",
    "* eta0 = no afecta mucho pero parece ser que con ser >1 ya esta\n",
    "* power_t = parecen ser practicamente iguales\n",
    "* n_iter = cuanto mas iter NO mejora el score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371161"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[CV] algorithm=adam, hidden_layer_sizes=(50,), activation=logistic, power_t=0.001, alpha=0.1, learning_rate=adaptive \n",
      "[CV] algorithm=adam, hidden_layer_sizes=(50,), activation=logistic, power_t=0.001, alpha=0.1, learning_rate=adaptive \n",
      "[CV] algorithm=adam, hidden_layer_sizes=(50,), activation=logistic, power_t=0.001, alpha=0.1, learning_rate=adaptive \n",
      "[CV] algorithm=adam, hidden_layer_sizes=(50,), activation=logistic, power_t=0.0005, alpha=0.1, learning_rate=adaptive \n",
      "Iteration 1, loss = 0.20405210\n",
      "Iteration 1, loss = 0.20380843\n",
      "Iteration 1, loss = 0.20378706\n",
      "Iteration 1, loss = 0.20405210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teo/Repositories/scikit-learn/sklearn/metrics/classification.py:1097: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  algorithm=adam, hidden_layer_sizes=(50,), activation=logistic, power_t=0.0005, alpha=0.1, learning_rate=adaptive, score=0.000000 -   1.5s\n",
      "[CV] algorithm=adam, hidden_layer_sizes=(50,), activation=logistic, power_t=0.0005, alpha=0.1, learning_rate=adaptive \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teo/Repositories/scikit-learn/sklearn/metrics/classification.py:1097: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  algorithm=adam, hidden_layer_sizes=(50,), activation=logistic, power_t=0.001, alpha=0.1, learning_rate=adaptive, score=0.000000 -   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teo/Repositories/scikit-learn/sklearn/metrics/classification.py:1097: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  algorithm=adam, hidden_layer_sizes=(50,), activation=logistic, power_t=0.001, alpha=0.1, learning_rate=adaptive, score=0.000000 -   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teo/Repositories/scikit-learn/sklearn/metrics/classification.py:1097: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  algorithm=adam, hidden_layer_sizes=(50,), activation=logistic, power_t=0.001, alpha=0.1, learning_rate=adaptive, score=0.000000 -   1.9s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-3491f77d2a02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m verbose=3, pre_dispatch='2*n_jobs', error_score='raise')\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mpredicted_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \"\"\"\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    558\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 560\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m                 for train, test in cv)\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    808\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[1;31m# a working pool as they expect.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X_train,X_test,y_train ,y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = { 'alpha': [1e-1,5*1e-2,1e-2],\n",
    "              'hidden_layer_sizes':[(50,),(15,5)],\n",
    "              'learning_rate': ['adaptive',\"invscaling\"],\n",
    "              \"algorithm\": ['adam'],'momentum': [1e-2, 1e-1, 0.5],\n",
    "  'power_t': [1e-3, 5*1e-4, 1e-5], 'activation':['logistic','relu']\n",
    " }\n",
    "\n",
    "mlp = MLPClassifier(shuffle=True, \n",
    "                 verbose=True)\n",
    "\n",
    "clf =GridSearchCV(mlp, param_grid, scoring='roc_auc', fit_params=None, n_jobs=4, iid=True, refit=True, \n",
    "verbose=3, pre_dispatch='2*n_jobs', error_score='raise')\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_test).max(axis=1)\n",
    "predicted_proba['gt'] = y_test\n",
    "predicted_proba['class'] = clf.predict(X_test)\n",
    "\n",
    "threshold = 0.7\n",
    "ind = predicted_proba['prob']>threshold\n",
    "\n",
    "all_time =   time.time() - start_time\n",
    "\n",
    "print(classification_report(predicted_proba.loc[ind,'gt'].values,predicted_proba.loc[ind,'class'].values ))\n",
    "\n",
    "cv_result = pd.DataFrame(columns=['mean_score' ] + [key for key, value in clf.grid_scores_[0].parameters.items() ] )\n",
    "i=0\n",
    "for setup in clf.grid_scores_:\n",
    "    row = [clf.grid_scores_[i].mean_validation_score ] + [value for key, value in clf.grid_scores_[i].parameters.items() ]\n",
    "    cv_result.loc[i] = row\n",
    "    i+=1\n",
    "#este flag es basicamente para chequear que hayamos terminado nomas\n",
    "finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2d39bdb4c6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alpha\n",
       "0.00     0.000007\n",
       "0.01     0.000007\n",
       "0.10     0.000007\n",
       "0.50     0.000007\n",
       "1.00     0.000007\n",
       "10.00    0.000007\n",
       "Name: mean_score, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare best parameters to tune\n",
    "coln=1\n",
    "cv_result.groupby(cv_result.columns[coln])['mean_score'].mean()\n",
    "cv_result.groupby(cv_result.columns[coln])['mean_score'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results\n",
    "* activation = logistic es 10% mejor\n",
    "* alpha = 1e-2 el mejor \n",
    "* power_t = cuanto mas chico mejor, 1e-3 por lo menos\n",
    "* hidden_layer_size = menos layers es mejor..?\n",
    "* power_t = parecen ser practicamente iguales\n",
    "* n_iter = cuanto mas iter NO mejora el score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli RBM features selection & Logit crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-8675e4c7d9ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m verbose=0, pre_dispatch='2*n_jobs', error_score='raise')\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#este metodo NO tiene predicted proba, lo que hacemos es recorrer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \"\"\"\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    558\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 560\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m                 for train, test in cv)\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    808\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[1;31m# a working pool as they expect.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "## obs. este metodo es especial y asume que todos los valores son True/False o que \\in [0,1]\n",
    "# luego tengo que pensar en como tomar los features nuevamente.\n",
    "# tampoco 'fittea' en la forma tradicional. Sino que se le puede tomar al y como un feature mas y esta red\n",
    "# va 'modificando' todos los valores del X (minimizando la entropia) para dar un output. Luego corriendo \n",
    "# clf.gibbs(X_test) con el y_test como feature tmb, nos transforma la data para ver el output como la 'prediccion'\n",
    "# obviamente no tiene probabilidades\n",
    "\n",
    "\n",
    "#X = X_train[X_cols].values\n",
    "#y = X_train['ground_truth'].values\n",
    "\n",
    "df = X_train.drop(X_train[X_train[X_train.columns[0:3]].\\\n",
    "                                   sum(axis=1)==0].index)\n",
    "df = df[X_cols + ['ground_truth']]\n",
    "\n",
    "for col in X_train.columns[0:3]:\n",
    "    df[col] = df[col]*1.0/df[df.columns[0:3]].sum(axis=1)\n",
    "\n",
    "df[df.columns[3]] =  df[df.columns[3]]/df[df.columns[3]].max()\n",
    "df[df.columns[4]] =  df[df.columns[4]]/df[df.columns[4]].max()\n",
    "\n",
    "X = df[df.columns[:-1]].values\n",
    "y = df['ground_truth'].values\n",
    "\n",
    "\n",
    "param_grid = {'rbm__n_components': [256, 128,46,10],\n",
    "   'rbm__n_iter':[15,10,5], 'rbm__learning_rate': [1e-4,1e-3,1e-2,1e-1,5*1e-3,5*1e-2,5*1e-1],\n",
    "  'rbm__batch_size': [10e4,3*10e3, 1e3, 300],\"logistic__C\": [1.0, 10.0, 100.0] \n",
    " }\n",
    "\n",
    "rbm = BernoulliRBM(verbose=True)\n",
    "logistic = LogisticRegression()\n",
    "classifier = Pipeline([(\"rbm\", rbm), (\"logistic\", logistic)])\n",
    "\n",
    "X_train,X_test,y_train ,y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf =GridSearchCV(classifier, param_grid, scoring='roc_auc', fit_params=None, n_jobs=4, iid=True, refit=True, \n",
    "verbose=3, pre_dispatch='2*n_jobs', error_score='raise')\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#este metodo NO tiene predicted proba, lo que hacemos es recorrer \n",
    "#predicted_labels = rbm.gibbs(X_test)[:,-1]\n",
    "#real_labels = X_test[:,-1]\n",
    "#print(classification_report(real_labels,predicted_labels ))\n",
    "\n",
    "\n",
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_test).max(axis=1)\n",
    "predicted_proba['gt'] = y_test\n",
    "predicted_proba['class'] = clf.predict(X_test)\n",
    "\n",
    "threshold = 0.7\n",
    "ind = predicted_proba['prob']>threshold\n",
    "print(classification_report(predicted_proba.loc[ind,'gt'].values,predicted_proba.loc[ind,'class'].values ))\n",
    "all_time =   time.time() - start_time\n",
    "\n",
    "finished = True\n",
    "\n",
    "cv_result = pd.DataFrame(columns=['mean_score' ] + [key for key, value in clf.grid_scores_[0].parameters.items() ] )\n",
    "i=0\n",
    "for setup in clf.grid_scores_:\n",
    "    row = [clf.grid_scores_[i].mean_validation_score ] + [value for key, value in clf.grid_scores_[i].parameters.items() ]\n",
    "    cv_result.loc[i] = row\n",
    "    i+=1\n",
    "    \n",
    "print('This cell took %s seconds to run' % all_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coln=1\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].mean())\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ADaboost CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-b1ab4c054abd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mada_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mada_gs_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mada_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mada_param_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mada_gs_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mada_gs_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \"\"\"\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    558\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 560\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m                 for train, test in cv)\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 sample_weight)\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \"\"\"\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    366\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf_param_grid = {'n_estimators': [10, 30, 100, 300, 1000]}\n",
    "boost_param_grid = {'n_estimators': [10, 30, 100, 300, 1000],\n",
    "                      'max_depth': [2, 3, 4, 5],\n",
    "                      'min_samples_leaf': [1, 2, 3]}\n",
    "ada_param_grid = {'n_estimators': [10, 30, 100, 300, 1000],\n",
    "                   'learning_rate': [0.1, 0.3, 1.0, 3.0]}\n",
    "\n",
    "X_train,X_test,y_train ,y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "#rf_est = RandomForestClassifier()\n",
    "#rf_gs_cv = GridSearchCV(rf_est, rf_param_grid).fit(X_train, Y_train)\n",
    "#print(rf_gs_cv.best_score_, rf_gs_cv.best_params_)\n",
    "print('\\n')\n",
    " \n",
    "#boost_est = GradientBoostingClassifier()\n",
    "#boost_gs_cv = GridSearchCV(boost_est, boost_param_grid).fit(X_train, y_train)\n",
    "#print(boost_gs_cv.best_score_, boost_gs_cv.best_params_)\n",
    "print('\\n')\n",
    " \n",
    "ada_est = AdaBoostClassifier()\n",
    "ada_gs_cv = GridSearchCV(ada_est, ada_param_grid).fit(X_train, y_train)\n",
    "print(ada_gs_cv.best_score_, ada_gs_cv.best_params_)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n",
      "[CV] n_estimators=200, learning_rate=0.0001 ..........................\n",
      "[CV] n_estimators=200, learning_rate=0.0001 ..........................\n",
      "[CV] n_estimators=200, learning_rate=0.0001 ..........................\n",
      "[CV] n_estimators=600, learning_rate=0.0001 ..........................\n",
      "[CV] n_estimators=600, learning_rate=0.0001 ..........................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-05e56bcecea4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m verbose=3, pre_dispatch='2*n_jobs', error_score='raise')\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mpredicted_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \"\"\"\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    558\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 560\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m                 for train, test in cv)\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    808\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/teo/Repositories/scikit-learn/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[1;31m# a working pool as they expect.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "param_grid = {'n_estimators': [200,600,60,45,30,15],\n",
    " 'learning_rate': [1e-4,1e-3,1e-2,1e-1,5*1e-3,5*1e-2,5*1e-1]\n",
    " }\n",
    "\n",
    "aboost = AdaBoostClassifier(algorithm='SAMME.R')\n",
    "\n",
    "clf =GridSearchCV(aboost, param_grid, scoring='roc_auc', fit_params=None, n_jobs=4, iid=True, refit=True, \n",
    "verbose=3, pre_dispatch='2*n_jobs', error_score='raise')\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_test).max(axis=1)\n",
    "predicted_proba['gt'] = y_test\n",
    "predicted_proba['class'] = clf.predict(X_test)\n",
    "\n",
    "threshold = 0.7\n",
    "ind = predicted_proba['prob']>threshold\n",
    "print(classification_report(predicted_proba.loc[ind,'gt'].values,predicted_proba.loc[ind,'class'].values ))\n",
    "all_time =   time.time() - start_time\n",
    "\n",
    "print('This cell took %s seconds to run' % all_time)\n",
    "\n",
    "finished = True\n",
    "#este flag es basicamente para chequear \n",
    "\n",
    "cv_result = pd.DataFrame(columns=['mean_score' ] + [key for key, value in clf.grid_scores_[0].parameters.items() ] )\n",
    "i=0\n",
    "for setup in clf.grid_scores_:\n",
    "    row = [clf.grid_scores_[i].mean_validation_score ] + [value for key, value in clf.grid_scores_[i].parameters.items() ]\n",
    "    cv_result.loc[i] = row\n",
    "    i+=1\n",
    "\n",
    "finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion\n",
      "gini    0.802919\n",
      "Name: mean_score, dtype: float64\n",
      "criterion\n",
      "gini    0.000598\n",
      "Name: mean_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coln=3\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].mean())\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossV SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "param_grid = [{'kernel': ['rbf'], 'gamma': list(10.0 ** np.arange(-2, 3)),\n",
    "                     'C': list(10.0 ** np.arange(0, 4))},\n",
    "                    {'kernel': ['poly'], 'C': list(10.0 ** np.arange(0, 4)), 'degree'[2,3,4]}]\n",
    "\n",
    "svc = SVC(shuffle=True, probability=True,decision_function_shape = 'ovr',\n",
    "           verbose=True, class_weight='balanced'\n",
    "          )\n",
    "\n",
    "clf =GridSearchCV(svc, param_grid, scoring='roc_auc', fit_params=None, n_jobs=4, iid=True, refit=True, \n",
    "verbose=3, pre_dispatch='2*n_jobs', error_score='raise')\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_test).max(axis=1)\n",
    "predicted_proba['gt'] = y_test\n",
    "predicted_proba['class'] = clf.predict(X_test)\n",
    "\n",
    "threshold = 0.7\n",
    "ind = predicted_proba['prob']>threshold\n",
    "\n",
    "all_time =   time.time() - start_time\n",
    "\n",
    "print(classification_report(predicted_proba.loc[ind,'gt'].values,predicted_proba.loc[ind,'class'].values ))\n",
    "\n",
    "cv_result = pd.DataFrame(columns=['mean_score' ] + [key for key, value in clf.grid_scores_[0].parameters.items() ] )\n",
    "i=0\n",
    "for setup in clf.grid_scores_:\n",
    "    row = [clf.grid_scores_[i].mean_validation_score ] + [value for key, value in clf.grid_scores_[i].parameters.items() ]\n",
    "    cv_result.loc[i] = row\n",
    "    i+=1\n",
    "#este flag es basicamente para chequear que hayamos terminado nomas\n",
    "finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coln=3\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].mean())\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting took 117.26272392272949 seconds to run\n",
      "This cell took 117.26284193992615 seconds to run\n"
     ]
    }
   ],
   "source": [
    "#tomo tiempo para saber cuanto tarda fitear el estimador\n",
    "start_time = time.time()\n",
    "\n",
    "#model = model.fit(X,y,sample_weight=W)\n",
    "\n",
    "W = np.array([10 if i == 1 or i ==2  else 1 for i in y_mini])\n",
    "gradboost.fit(X_mini,y_mini, sample_weight=W)\n",
    "\n",
    "elapsed_time =   time.time() - start_time\n",
    "print('GradientBoosting took %s seconds to run' % elapsed_time)\n",
    "\n",
    "#validated =  cross_val_score(gradboost,X,y,cv=5, scoring = \"f1_weighted\")\n",
    "\n",
    "\n",
    "all_time =   time.time() - start_time\n",
    "print('This cell took %s seconds to run' % all_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timings\n",
    "* 5s con  10 n_estimadores, 15 max_depth y X.sample(0.2)\n",
    "* 15s con 10 n_estimadores, 15 max_depth y X.sample(0.2)\n",
    "* 47s con 10 n_estimadores, 15 max_depth y X.sample(0.4)\n",
    "* 35s con 10 n_estimadores, 15 max_depth y X.sample(0.4)\n",
    "* 117 con 20 n_estimadores, 20 max_depth y X.sample(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    128153\n",
       "1      7064\n",
       "Name: ground_truth, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['ground_truth'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'criterion': ['gini'], 'n_estimators': [15,30,50],\n",
    "  'max_features': [\"auto\", \"log2\"], \"bootstrap\": [ False, True],\n",
    "    \"min_samples_leaf\": np.append(np.random.randint(3,15,3),[3]),'max_depth':[10,20,30], \n",
    "               \"class_weight\": ['balanced']\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced \n",
      "[CV] bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced \n",
      "[CV] bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced \n",
      "[CV] bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced \n",
      "[CV] bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced \n",
      "[CV] bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced \n",
      "[CV] bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced \n",
      "[CV] bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced \n",
      "[CV]  bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced, score=0.499320 - 4.0min\n",
      "[CV] bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced \n",
      "[CV]  bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced, score=0.501824 - 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  4.2min remaining:  -19.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced \n",
      "[CV]  bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced, score=0.500056 - 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  4.3min remaining:  -19.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced \n",
      "[CV]  bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced, score=0.498584 - 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  4.4min remaining:  -20.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced \n",
      "[CV]  bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced, score=0.500978 - 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  4.7min remaining:  -21.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=15, class_weight=balanced, score=0.500560 - 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  4.8min remaining:  -22.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced, score=0.499370 - 5.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  6.3min remaining:  -29.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced, score=0.499699 - 6.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  6.8min remaining:  -31.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced, score=0.502255 - 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  9.5min remaining:  -43.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=False, min_samples_leaf=6, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced, score=0.499331 - 5.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  9.7min remaining:  -44.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced, score=0.500189 - 5.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  9.7min remaining:  -44.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  bootstrap=False, min_samples_leaf=12, n_estimators=25, criterion=gini, max_features=auto, max_depth=25, class_weight=balanced, score=0.499681 - 5.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  12 | elapsed:  9.8min remaining:  -45.1s\n",
      "[Parallel(n_jobs=8)]: Done  12 out of  12 | elapsed:  9.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search took 1023.46121788 seconds to run\n",
      "Grid Search took 0.000302076339722 seconds to run\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.92      0.80     47125\n",
      "          1       0.29      0.08      0.13     19498\n",
      "\n",
      "avg / total       0.58      0.67      0.60     66623\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.77      0.74     94223\n",
      "          1       0.29      0.23      0.26     39023\n",
      "\n",
      "avg / total       0.59      0.61      0.60    133246\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.66      0.68    141157\n",
      "          1       0.30      0.35      0.32     58712\n",
      "\n",
      "avg / total       0.59      0.57      0.57    199869\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.57      0.63    188258\n",
      "          1       0.29      0.43      0.35     78234\n",
      "\n",
      "avg / total       0.59      0.53      0.55    266492\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.67      0.69    470763\n",
      "          1       0.29      0.33      0.31    195467\n",
      "\n",
      "avg / total       0.59      0.57      0.58    666230\n",
      "\n",
      "* Rates for distinct percentage groups \n",
      "\n",
      "10 %% group rate: 0.292661693409 size: 66623\n",
      "20 %% group rate: 0.292864326134 size: 133246\n",
      "30 %% group rate: 0.293752407827 size: 199869\n",
      "40 %% group rate: 0.293569788211 size: 266492\n",
      "This cell took 1046.18349791 seconds to run\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "param_grid = {'criterion': ['gini'], 'n_estimators': [25],\n",
    "  'max_features': [\"auto\"], \"bootstrap\": [ False],\n",
    "    \"min_samples_leaf\":[6,12],'max_depth':[15,25], \n",
    "               \"class_weight\": ['balanced']\n",
    "              }\n",
    "rforest  = RandomForestClassifier( )\n",
    "\n",
    "clf =GridSearchCV(rforest, param_grid, scoring='roc_auc', fit_params=None, n_jobs=8, iid=True, refit=True, \n",
    "verbose=3, pre_dispatch='2*n_jobs', error_score='raise')\n",
    "\n",
    "clf2 = RandomForestClassifier()\n",
    "\n",
    "X = X_train[X_train.columns[:-1]].values\n",
    "Y = X_train['y'].values\n",
    "\n",
    "#X = X_train_categorical\n",
    "clf.fit(X,Y)\n",
    "\n",
    "\n",
    "#random_search.fit(X_train,y_train)\n",
    "elapsed_time =  time.time() - start_time\n",
    "\n",
    "print('Random Search took %s seconds to run' % elapsed_time)\n",
    "\n",
    "\n",
    "all_time =   time.time() - start_time \n",
    "print('Grid Search took %s seconds to run' % (all_time - elapsed_time))\n",
    "\n",
    "\n",
    "X_test = X_val[X_train.columns[:-1]].values\n",
    "Y_test = X_val['y'].values\n",
    "\n",
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_test).max(axis=1)\n",
    "predicted_proba.index = X_val.index\n",
    "predicted_proba['class'] = clf.predict(X_test)\n",
    "predicted_proba['gt'] =  Y_test\n",
    "predicted_proba.index.name = \"USER\"\n",
    "predicted_proba.sort_values(by = 'prob',ascending = False, inplace = True)\n",
    "\n",
    "cut_percentages = [10,20,30,40]\n",
    "#converted = (predicted_proba['class']==1)\n",
    "#me fijo todos los users que van a convertir en el futuro\n",
    "\n",
    "past_epidemic = X_val[X_val['y']==1].index.values\n",
    "\n",
    "start = 0\n",
    "mobility_dict = {}\n",
    "index_segments = []\n",
    "for i in cut_percentages:\n",
    "    cut_size = int(len(predicted_proba)*i/100.0)    \n",
    "    bool_array = (~np.ones(len(predicted_proba), dtype=bool))\n",
    "    bool_array[start: start + cut_size] = True\n",
    "    index_segments += [bool_array]\n",
    "    start = start + cut_size\n",
    "    #subtable = \n",
    "    mobility_dict[str(i)] = predicted_proba.loc[index_segments[-1]].index.values\n",
    "    \n",
    "    print(classification_report(predicted_proba.loc[index_segments[-1],\"gt\"].values,\n",
    "                            predicted_proba.loc[index_segments[-1],'class'].values ))\n",
    "    \n",
    "    \n",
    "print(classification_report(predicted_proba.loc[:,\"gt\"].values,\n",
    "                            predicted_proba.loc[:,'class'].values ))\n",
    "    \n",
    "    \n",
    "print(\"* Rates for distinct percentage groups \\n\")\n",
    "for i in cut_percentages:\n",
    "    subtable = mobility_dict[str(i)]\n",
    "\n",
    "    print(\"{0} %% group rate: {1} size: {2}\".format(str(i), \n",
    "                                np.in1d(subtable,past_epidemic).sum()*1.0/len(subtable),len(subtable))) \n",
    "    \n",
    "\n",
    "clf2.set_params(n_jobs = 6,**clf.best_params_)\n",
    "clf2.fit(X,Y)\n",
    "    \n",
    "cv_result = pd.DataFrame(columns=['mean_score' ] + [key for key, value in clf.grid_scores_[0].parameters.items() ] )\n",
    "i=0\n",
    "for setup in clf.grid_scores_:\n",
    "    row = [clf.grid_scores_[i].mean_validation_score ] + [value for key, value in clf.grid_scores_[i].parameters.items() ]\n",
    "    cv_result.loc[i] = row\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "    \n",
    "all_time =   time.time() - start_time\n",
    "\n",
    "#este flag es basicamente para chequear que hayamos terminado nomas\n",
    "finished = True\n",
    "\n",
    "print('This cell took %s seconds to run' % all_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.70631558433771058, 0.29368441566228937)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(Y)[0]*1.0/np.bincount(Y).sum(), np.bincount(Y)[1]*1.0/np.bincount(Y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
       "            criterion='gini', max_depth=25, max_features='auto',\n",
       "            max_leaf_nodes=None, min_samples_leaf=12, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=6,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier()\n",
    "clf2.set_params(n_jobs = 6,**clf.best_params_)\n",
    "clf2.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#OJO esta parte con poly = True no funciona.\n",
    "feature_importance = pd.DataFrame(clf2.feature_importances_,columns=['prob'])\n",
    "feature_importance = feature_importance.sort_values(by = 'prob',ascending=False)\n",
    "selected_features = X_train.columns[feature_importance.index]\n",
    "\n",
    "#guardo los labels como estan en las tablas (con numeros y no palabras)\n",
    "feature_importance['labels'] = selected_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047771</td>\n",
       "      <td>ANTENNA_ID_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047296</td>\n",
       "      <td>ANTENNA_ID_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.046120</td>\n",
       "      <td>ANTENNA_ID_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045383</td>\n",
       "      <td>ANTENNA_ID_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.045289</td>\n",
       "      <td>MOBILITY_DIAMETER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.043635</td>\n",
       "      <td>ANTENNA_ID_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.040790</td>\n",
       "      <td>ANTENNA_ID_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.040621</td>\n",
       "      <td>TimeWeekDaylight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.040533</td>\n",
       "      <td>ANTENNA_ID_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.038278</td>\n",
       "      <td>ANTENNA_ID_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.038017</td>\n",
       "      <td>TimeWeekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.037330</td>\n",
       "      <td>TimeWeekNight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.036551</td>\n",
       "      <td>ANTENNA_ID_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.035400</td>\n",
       "      <td>ANTENNA_ID_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033911</td>\n",
       "      <td>COUNT_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028475</td>\n",
       "      <td>COUNT_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.027249</td>\n",
       "      <td>CallsWeekDaylight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.025336</td>\n",
       "      <td>COUNT_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.024330</td>\n",
       "      <td>TimeWeekDaylight_EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.024236</td>\n",
       "      <td>CallsWeekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.023595</td>\n",
       "      <td>CallsWeekNight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.023021</td>\n",
       "      <td>COUNT_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.022360</td>\n",
       "      <td>TimeWeekend_EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.021073</td>\n",
       "      <td>COUNT_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.020677</td>\n",
       "      <td>TimeWeekNight_EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.016019</td>\n",
       "      <td>COUNT_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.015685</td>\n",
       "      <td>TOTAL_USERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.015399</td>\n",
       "      <td>CallsWeekDaylight_EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.014447</td>\n",
       "      <td>COUNT_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.013324</td>\n",
       "      <td>COUNT_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.012807</td>\n",
       "      <td>CallsWeekend_EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.012719</td>\n",
       "      <td>COUNT_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.012291</td>\n",
       "      <td>COUNT_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.012157</td>\n",
       "      <td>CallsWeekNight_EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.008414</td>\n",
       "      <td>EPI_USERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.005324</td>\n",
       "      <td>EXP_USERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.004137</td>\n",
       "      <td>EPIDEMIC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        prob                 labels\n",
       "2   0.047771           ANTENNA_ID_1\n",
       "0   0.047296           ANTENNA_ID_0\n",
       "6   0.046120           ANTENNA_ID_3\n",
       "4   0.045383           ANTENNA_ID_2\n",
       "20  0.045289      MOBILITY_DIAMETER\n",
       "8   0.043635           ANTENNA_ID_4\n",
       "10  0.040790           ANTENNA_ID_5\n",
       "25  0.040621       TimeWeekDaylight\n",
       "12  0.040533           ANTENNA_ID_6\n",
       "14  0.038278           ANTENNA_ID_7\n",
       "27  0.038017            TimeWeekend\n",
       "26  0.037330          TimeWeekNight\n",
       "16  0.036551           ANTENNA_ID_8\n",
       "18  0.035400           ANTENNA_ID_9\n",
       "1   0.033911                COUNT_0\n",
       "3   0.028475                COUNT_1\n",
       "22  0.027249      CallsWeekDaylight\n",
       "5   0.025336                COUNT_2\n",
       "31  0.024330   TimeWeekDaylight_EPI\n",
       "24  0.024236           CallsWeekend\n",
       "23  0.023595         CallsWeekNight\n",
       "7   0.023021                COUNT_3\n",
       "33  0.022360        TimeWeekend_EPI\n",
       "9   0.021073                COUNT_4\n",
       "32  0.020677      TimeWeekNight_EPI\n",
       "11  0.016019                COUNT_5\n",
       "34  0.015685            TOTAL_USERS\n",
       "28  0.015399  CallsWeekDaylight_EPI\n",
       "13  0.014447                COUNT_6\n",
       "15  0.013324                COUNT_7\n",
       "30  0.012807       CallsWeekend_EPI\n",
       "17  0.012719                COUNT_8\n",
       "19  0.012291                COUNT_9\n",
       "29  0.012157     CallsWeekNight_EPI\n",
       "35  0.008414              EPI_USERS\n",
       "36  0.005324              EXP_USERS\n",
       "21  0.004137               EPIDEMIC"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600644, 38), (1600644,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600644, 38)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
       "             criterion='gini', max_depth=15, max_features='auto',\n",
       "             max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=35, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False), 0.54557839409818476)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = classification_report(predicted_proba.loc[ind,'y'].values,predicted_proba.loc[ind,'class'].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators\n",
      "20.0    0.537925\n",
      "35.0    0.541880\n",
      "Name: mean_score, dtype: float64\n",
      "n_estimators\n",
      "20.0    0.00539\n",
      "35.0    0.00419\n",
      "Name: mean_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coln = 3\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].mean())\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "* NO escalar (normalizar, restar la media  dividir por la std, etc) los resultados pues REVIENTAN los scores.\n",
    "* bootstrap  = False es 5% mejor\n",
    "* min_samples_leaf = mas chico es claramente mejor, pero tmb aumenta el overfitting lo cual me hace caer mucho el valor del recall en el test_set. Sin embargo es un parametro muy sensible en la precision. Resta evaluar asi el tradeoff entre la precision y el volumen de users al cual queremos llegar.\n",
    "* n_estimators = aumentar mas de 30 no tendria mucho sentido\n",
    "* citerion = entropy o gini no cambia. gini podria ser mejor entonces pues entropy usa logs de los valores lo cual es mas computacionalmente costoso\n",
    "* max_features = no afecta al score. con auto esta bien\n",
    "* max_depth =  mas es mejor. intentaria probar con >15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Virtualenvs/mateo/lib/python3.4/site-packages/sklearn/metrics/classification.py:1117: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00     26720\n",
      "          1       0.25      1.00      0.40      8738\n",
      "\n",
      "avg / total       0.06      0.25      0.10     35458\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.43      0.60     66920\n",
      "          1       0.07      0.76      0.14      3996\n",
      "\n",
      "avg / total       0.92      0.45      0.57     70916\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99    105255\n",
      "          1       0.00      0.00      0.00      1119\n",
      "\n",
      "avg / total       0.98      0.99      0.98    106374\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99    139582\n",
      "          1       0.00      0.00      0.00      2250\n",
      "\n",
      "avg / total       0.97      0.98      0.98    141832\n",
      "\n",
      "Rates for dafiti app 189 \n",
      "10 %% group rate: 0.10429240227875233 size: 35458\n",
      "20 %% group rate: 0.024479666083817474 size: 70916\n",
      "30 %% group rate: 0.009494801361234888 size: 106374\n",
      "40 %% group rate: 0.014101190140447854 size: 141832\n"
     ]
    }
   ],
   "source": [
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_train)[:,1]\n",
    "predicted_proba['class'] = clf.predict(X_train)\n",
    "predicted_proba['ref_hash'] =  train_table.index.values\n",
    "predicted_proba['y'] =  y_train\n",
    "predicted_proba.sort_values(by = 'prob',ascending = False, inplace = True)\n",
    "\n",
    "cut_percentages = [10,20,30,40]\n",
    "#converted = (predicted_proba['class']==1)\n",
    "#me fijo todos los users que van a convertir en el futuro\n",
    "past_epidemic = test_table[test_table['y']>0]['ref_hash'].values\n",
    "\n",
    "\n",
    "start = 0\n",
    "mobility_dict = {}\n",
    "index_segments = []\n",
    "for i in cut_percentages:\n",
    "    cut_size = int(len(predicted_proba)*i/100.0)    \n",
    "    bool_array = (~np.ones(len(predicted_proba), dtype=bool))\n",
    "    bool_array[start: start + cut_size] = True\n",
    "    index_segments += [bool_array]\n",
    "    start = start + cut_size\n",
    "#    subtable = \n",
    "    mobility_dict[str(i)] = predicted_proba.loc[index_segments[-1],'ref_hash']\n",
    "    \n",
    "    print(classification_report(predicted_proba.loc[index_segments[-1],'y'].values,\n",
    "                            predicted_proba.loc[index_segments[-1],'class'].values ))\n",
    "    \n",
    "print(\"Rates for %s app %s \" % (appid_name[application_id], application_id))   \n",
    "for i in cut_percentages:\n",
    "    subtable = mobility_dict[str(i)]\n",
    "    \n",
    "    print(\"{0} %% group rate: {1} size: {2}\".format(str(i), subtable.isin(past_epidemic).sum()*1.0/len(subtable), \n",
    "                                       len(subtable))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha\n",
      "0.00     0.000000\n",
      "0.01     0.242260\n",
      "0.10     0.242266\n",
      "0.50     0.242220\n",
      "1.00     0.242200\n",
      "10.00    0.241269\n",
      "Name: mean_score, dtype: float64\n",
      "alpha\n",
      "0.00     0.000000\n",
      "0.01     0.017568\n",
      "0.10     0.017598\n",
      "0.50     0.017578\n",
      "1.00     0.017570\n",
      "10.00    0.017780\n",
      "Name: mean_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coln = 2\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].mean())\n",
    "print(cv_result.groupby(cv_result.columns[coln])['mean_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validate MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.85      0.91     70323\n",
      "          1       0.12      0.73      0.20      1932\n",
      "\n",
      "avg / total       0.97      0.84      0.89     72255\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.85      0.91     70323\n",
      "          1       0.12      0.73      0.20      1932\n",
      "\n",
      "avg / total       0.97      0.84      0.89     72255\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.85      0.91     70323\n",
      "          1       0.12      0.73      0.20      1932\n",
      "\n",
      "avg / total       0.97      0.84      0.89     72255\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.85      0.91     70323\n",
      "          1       0.12      0.73      0.20      1932\n",
      "\n",
      "avg / total       0.97      0.84      0.89     72255\n",
      "\n",
      "This cell took 1.06442093849 seconds to run\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# pero tenemos que transformar a y en un vector canonico indicando en 1 en la posicion correspondiente a su categoria\n",
    "#y =  label_binarize(X_train['ground_truth'].values.astype(int),\n",
    "#                    classes=list(set(X_train['ground_truth'].values.astype(int))))\n",
    "\n",
    "perc = 0.3\n",
    "mini_ind = np.random.choice(X_train.shape[0],int(perc*X_train.shape[0]),replace=False)\n",
    "#X_mini =  X[mini_ind,:]\n",
    "#y_mini = y[mini_ind]\n",
    "\n",
    "\n",
    "clf = MultinomialNB(\n",
    "        )\n",
    "\n",
    "clf2 = MultinomialNB()\n",
    "\n",
    "validated = []\n",
    "# Only take the first fold.\n",
    "n_folds = 4\n",
    "for i in range(n_folds):\n",
    "    # Break up the dataset into non-overlapping training (75%) and testing\n",
    "    # (25%) sets.\n",
    "    skf = StratifiedKFold(y_train, n_folds=n_folds)\n",
    "    train_index, test_index = next(iter(skf))\n",
    "\n",
    "    X_traincv = X_train[train_index]\n",
    "    y_traincv = y_train[train_index]\n",
    "\n",
    "    n_classes = len(np.unique(y_traincv))\n",
    "\n",
    "    clf.fit(X_traincv,y_traincv)\n",
    "    predictions = clf.predict(X_traincv)\n",
    "        \n",
    "    expected = y_traincv\n",
    "\n",
    "    print(classification_report(expected, predictions))\n",
    "\n",
    "\n",
    "predicted_proba = pd.DataFrame()\n",
    "predicted_proba['prob'] = clf.predict_proba(X_test).max(axis=1)\n",
    "predicted_proba['y'] = y_test\n",
    "predicted_proba['class'] = clf.predict(X_test)\n",
    "\n",
    "clf2.set_params(n_jobs = 6,**clf.best_params_)\n",
    "clf2.fit(X,Y)\n",
    "\n",
    "\n",
    "#este flag es basicamente para chequear que hayamos terminado nomas\n",
    "finished = True\n",
    "\n",
    "all_time =   time.time() - start_time\n",
    "print('This cell took %s seconds to run' % all_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.87      0.93    185744\n",
      "          1       0.15      0.73      0.26      5906\n",
      "\n",
      "avg / total       0.96      0.87      0.91    191650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.99\n",
    "ind = predicted_proba['prob']>threshold\n",
    "print(classification_report(predicted_proba.loc[ind,'y'].values,predicted_proba.loc[ind,'class'].values ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC took 362.8217749595642 seconds to run\n",
      "This cell took 362.8219118118286 seconds to run\n"
     ]
    }
   ],
   "source": [
    "#tomo tiempo para saber cuanto tarda fitear el estimador\n",
    "start_time = time.time()\n",
    "\n",
    "svc2.fit(X_mini,y_mini)\n",
    "elapsed_time =   time.time() - start_time\n",
    "print('SVC took %s seconds to run' % elapsed_time)\n",
    "\n",
    "all_time =   time.time() - start_time\n",
    "print('This cell took %s seconds to run' % all_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timings\n",
    "* 40k samples : 360s\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit took 1073.0756621360779 seconds to run\n"
     ]
    }
   ],
   "source": [
    "start_time =  time.time()\n",
    "lr.fit(X_train,y_train)\n",
    "elapsed_time =   time.time() - start_time\n",
    "print('Logit took %s seconds to run' % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit took 89.15975689888 seconds to run\n",
      "Linear SVC took 804.7178750038147 seconds to run"
     ]
    }
   ],
   "source": [
    "#tomo tiempo para saber cuanto tarda fitear el estimador\n",
    "start_time = time.time()\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "elapsed_time =   time.time() - start_time\n",
    "print('Logit took %s seconds to run' % elapsed_time)\n",
    "\n",
    "svc.fit(X_train,y_train)\n",
    "elapsed_time =   time.time() - start_time\n",
    "print('Linear SVC took %s seconds to run' % elapsed_time)\n",
    "\n",
    "all_time =   time.time() - start_time\n",
    "print('This cell took %s seconds to run' % all_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Todo\n",
    "* evaluate hit_rate and \n",
    "* tune adaboost, bernoulliRBM\n",
    "* xgboost\n",
    "* libffm\n",
    "* SVC muy lento.. speed up in AWS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum_Links files exploration and processing\n",
    "La idea es levantar atributos para el dataset estandar\n",
    "\n",
    "* observar estructura de la tabla \n",
    "* levantar solo los pares (i,j) donde i & j son users de la telco. Para saber si son users de la teclo es necesario levantar el file del hash_map (user_hash -- > int). Descartando asi el resto de los users.\n",
    "* extraer features de uso \"endemico\" para nuestros users de la TelCo\n",
    "\n",
    "## definiciones utilizadas\n",
    "* timestamp arranca en 0 segundos para 01/01/2012 00:00am\n",
    "* La Direction, viene dada relativa al user de la primer columna (incoming si entra y viceversa si es outgoing).\n",
    "\n",
    "### el dataset vendria con este header\n",
    " {'LineKeyOrigin':hash_object \\\\\\  'LineKeyTarget':np.uint32 \\\\\\  'CallsWeekDayLight':np.uint8 \\\\\\   'CallsWeekNight':np.uint8 \\\\\\  'CallsWeekend':np.uint8 \\\\\\  'TimeWeekDaylight':np.uint8 \\\\\\  'TimeWeekNight':np.uint8 \\\\\\  'TimeWeekend':np.uint8 \\\\\\  'SmsWeekDaylight':np.uint8 \\\\\\  'SmsWeekNight ':np.uint8 \\\\\\  'SmsWeekend':np.uint8} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd; import numpy as np; import os;import random;\n",
    "pd.set_option('display.max_rows', 300)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import unicodedata\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "antennas= pd.read_csv('antennas_mexico.csv',\n",
    "                      sep = \"|\",\n",
    "                      header=0,\n",
    "                      index_col=0)\n",
    "antennas.index.rename(\"AntennaID\",inplace=True)\n",
    "\n",
    "#en user_dict va a quedar el mapping de hashes por usuario a su int correspondiente.\n",
    "\n",
    "users_dict= pd.read_csv('hash_user_dict_mexico.csv.gz',\n",
    "                       sep = \"|\",\n",
    "                       header=0,\n",
    "                       index_col=0).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input  = \"/grandata/voice/201401_geo/GEO\\ Voz\\ Pospago\\ 01-02012014.txt.gz\"\n",
    "!zcat $test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#seteamos el lugar de trabajo\n",
    "rootdir=\"/grandata/voice/sum_links\"\n",
    "os.chdir(rootdir)\n",
    "year = \"2015\"; \n",
    "month_start= \"04\";\n",
    "#todos estos sum_links terminan 2 meses mas tardes\n",
    "month_end= \"0\"+str(int(month_start)+2)\n",
    "input_file= rootdir +\"/sum_links_{y}{ms}_{y}{me}.txt.gz\".format(y=year,ms=month_start,me=month_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/grandata/voice/sum_links/sum_links_201504_201506.txt.gz'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LineKeyOrigin|LineKeyTarget|CallsWeekDaylight|CallsWeekNight|CallsWeekend|TimeWeekDaylight|TimeWeekNight|TimeWeekend|SmsWeekDaylight|SmsWeekNight|SmsWeekend\r\n",
      "CD5844F7EB2C00C63465E43B44EB9025|3F6FEB3BA73D7E51D906B19F1E0A9099|0|0|0|0|0|0|1|0|0\r\n",
      "6AEE0F059A39A7E1CF90B93B5BE8E5AD|3F6FEB3BA73D7E51D906B19F1E0A9099|0|0|0|0|0|0|0|0|3\r\n",
      "CD5844F7EB2C00C63465E43B44EB9025|21F587020D72C34131DDD04C0FCD6CFF|0|0|0|0|0|0|0|0|1\r\n",
      "CD5844F7EB2C00C63465E43B44EB9025|796D132ECDF757CBCE8FB01DDA3891D6|0|0|0|0|0|0|1|0|0\r\n",
      "5E8BADC0F256DEC1ACBB1AB2B77527E9|796D132ECDF757CBCE8FB01DDA3891D6|0|0|3|0|0|203|0|0|0\r\n",
      "CD5844F7EB2C00C63465E43B44EB9025|5FA3B22C222AFFF5DFAAE7CFCAEBCA30|0|0|0|0|0|0|0|0|1\r\n",
      "6B56A956E1A9E56BE4D90543C8706E76|5FA3B22C222AFFF5DFAAE7CFCAEBCA30|0|0|0|0|0|0|1|0|0\r\n",
      "2752F0E0F8226FFF2A3189EB131330AC|46B254E4C1A98CECB4F3D52E66BB836D|1|0|1|196|0|36|0|0|0\r\n",
      "2752F0E0F8226FFF2A3189EB131330AC|A954CAD047A827B6DEBA10BD797E7B16|10|4|9|1403|208|1416|0|0|0\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zcat $input_file | head -n10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asumimos...\n",
    "* los calls son un conteo de quantity.\n",
    "* el time se refiere a cantidad de segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##leemos alguna tabla ejemplo\n",
    "tabla2 = pd.read_csv(input_file,\n",
    "                     sep = \"|\", \n",
    "              names = ['LineKeyOrigin', 'LineKeyTarget', 'CallsWeekDaylight', 'CallsWeekNight',\n",
    "                       'CallsWeekend','TimeWeekDaylight', 'TimeWeekNight', 'TimeWeekend',\n",
    "                       'SmsWeekDaylight', 'SmsWeekNight', 'SmsWeekend'],\n",
    "                     usecols=\n",
    "                             engine = 'c',index_col=None, lineterminator='\\n', skipinitialspace=True, \n",
    "                    header=None,dtype = {'Target':np.int32,'Destination':np.uint32,'TimeStamp':np.uint32,\n",
    "                                      'Duration':np.uint16,'AntennaID':np.uint16,'Direction':np.object_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tabla.dtypes, tabla2.dtypes\n",
    "#tabla['Destination'] = tabla['Destination'].astype(np.uint32,copy=False);\n",
    "#tabla['Target'] = tabla['Target'].astype(np.uint32,copy=False);\n",
    "#tabla['TimeStamp'] = tabla['TimeStamp'].astype(np.uint32,copy=False);\n",
    "#tabla['Duration'] = tabla['Duration'].astype(np.uint16,copy=False);\n",
    "#tabla['AntennaID'] = tabla['AntennaID'].astype(np.uint16,copy=False);\n",
    "#tabla['Direction'] = tabla['Direction'].astype(np.object_,copy=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tabla.sort_values(by='TimeStamp',ascending=True,inplace=True)\n",
    "tabla2.sort_values(by='TimeStamp',ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tabla2.sort_values(by='TimeStamp',ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#agarro una muestra de 300k para hacer pruebas\n",
    "sample=tabla.ix[random.sample(tabla.index, 3*(10**5))]\n",
    "sample2 = tabla.ix[random.sample(tabla.index, 3*(10**5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#enriquezco el dataset con la info de antennas_mexico.csv\n",
    "enriched_sample = pd.merge(sample,antennas,left_on=\"AntennaID\",right_index=True)\n",
    "enriched_sample2 = pd.merge(sample2,antennas,left_on=\"AntennaID\",right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enriched_sample.sort_values(by='TimeStamp',ascending=True,inplace=True)\n",
    "enriched_sample2.sort_values(by='TimeStamp',ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enriched_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Mobility\n",
    "La idea es ahora mirar dentro de la base (prototipamos con sample) la lista de antenas utilizadas por cada usuario para obtener la variable dependiente $Y$ que vendria a ser si el tipo viajo o no por la zona endemica (visto como un llamado basicamente desde una antena en la epidemic zone). Despues buscaremos atributos, o variables regresoras que logren predecir ese comportamiento en el pasado pero usando solo los atributos en un timeframe \"del futuro\". Busco atributos que correlacionen con 'haber viajado por la zona endemica en el pasado'\n",
    "#### pseudocode:\n",
    "* tomamos el set de origins/targets de la tabla.\n",
    "* p/c/user lo mapeamos a un conjunto de antenas que sabemos que el tipo uso en algun CDR. Aca usamos alguna operacion split, apply, combine dondeset donde el key va a ser la columna origin y la data es AntennaID. Ahi extraemos la lista de antenas que el user uso. \n",
    "* descartamos del dataset los users \"vacios\". i.e. aquellos que su conjunto de antenas da vacio. Para esto descartamos todas las filas que no tienen ningun user (origin o target) en la tabla split/apply/combine. Asi obtendremos una lista de usuarios de la telco que viajaron por mexico a partir de sus llamados en ese mes. Despues solo hacer un outer join de todas estas tablas resultantes para cada mes. Asi como tambien un join de estos \"sets\" de antennas por user. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = tabla['AntennaID'].groupby(tabla['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#con esto extraigo el mapping user --> antennas used that month\n",
    "user_antenna_map = sample.groupby('Target')['AntennaID'].apply(lambda x: x.tolist()).to_dict();\n",
    "user_antenna_map2 = sample2.groupby('Target')['AntennaID'].apply(lambda x: x.tolist()).to_dict();\n",
    "\n",
    "#esto es 'tipo' un head para dicts\n",
    "dict(user_antenna_map.items()[0:10]), dict(user_antenna_map2.items()[0:10])\n",
    "\n",
    "from collections import defaultdict\n",
    "user_antenna_map_concat = defaultdict(list)\n",
    "\n",
    "#Veo como concatenar dos \"maps\" distintos y dejando solo las antenas unicas..\n",
    "for d in (user_antenna_map, user_antenna_map2): # podemos meter cuantos dicts queramos aca..\n",
    "    for key, value in d.iteritems():\n",
    "        #list(set()) elimina repetidos\n",
    "        user_antenna_map_concat[key] = list(set(user_antenna_map_concat[key] + value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## scripot para convertir las columnas de otros types --> ints\n",
    "\n",
    "passes=5\n",
    "\n",
    "#ver el tiempo que tarda\n",
    "now = time.time()\n",
    "\n",
    "#el chunk basicamente va leyendo el file de a 'chunksize' cantidad de filas\n",
    "#subgroup = pd.DataFrame()\n",
    "\n",
    "for group in range(passes):\n",
    "    subgroup = pd.DataFrame()\n",
    "    table = pd.read_csv(\n",
    "            input_file,\n",
    "            engine = 'c',\n",
    "            chunksize = 5*10**7,\n",
    "#            iterator =True,\n",
    "            sep = ' ',\n",
    "            header = None,\n",
    "            index_col=None,\n",
    "            names = ['Target','Destination','Direction','TimeStamp','Duration','AntennaID'],\n",
    "            usecols = ['Target', 'AntennaID','TimeStamp'],\n",
    "            dtype = {'Target':np.uint32,'Destination':np.uint32,'TimeStamp':np.uint32,\n",
    "                                      'Duration':np.uint16,'AntennaID':np.uint16,'Direction':np.object_}\n",
    "            )\n",
    "    \n",
    "    #cuando entramos a este loop, table tiene tantos 'chunks' como el valor entero de la cantidad de lineas en el file\n",
    "    #dividido el tamanyo del chunksize\n",
    "    for chunk in table:\n",
    "            #a cada chunk filtro por todos los Targets que tengan resto group modulo passes. y trabajo sobre la tabla\n",
    "            #subgroup nada mas que ahora tiene pocos usuarios\n",
    "            chunk['Target_int'] = chunk[].map(users_dict)\n",
    "            subgroup = subgroup.append(chunk[chunk['Target'] % passes == group])\n",
    "    #\n",
    "    #entonces la idea es que yo ahora solo voy a trabajar, dentro de la tabla\n",
    "    \n",
    "    #paso de segundos a horas\n",
    "    #notar que timestamp arranca en 0 segundos para domingo 01/01/2012 00:00am \n",
    "    #con lo cual domingo es el dia 0, lunes el 1, asi..\n",
    "    \n",
    "    subgroup['Hour'] =  (subgroup['TimeStamp'].values*1.0/3600)%24\n",
    "    subgroup['Day']  =  (subgroup['TimeStamp'].values*1.0/(3600*24))%7\n",
    "    \n",
    "    #filtro usuarios con pocos o demasiados llamados en general menos de 5 mensuales y mas de 400  \n",
    "    insignificant_users = subgroup['Target'].value_counts()[(subgroup['Target'].value_counts() < 5) \\\n",
    "                        | (subgroup['Target'].value_counts() > 400)].index.values.tolist()\n",
    "    subgroup= subgroup.loc[~subgroup['Target'].isin(insignificant_users)]\n",
    "    \n",
    "    \n",
    "    #filtro segun nightfilter y week_end\n",
    "    if night_filter == True:\n",
    "        subgroup = subgroup.loc[(subroup['Hour']<8) | (subgroup['Hour']>18)]\n",
    "    if week_end == True:\n",
    "        subgroup = subgroup.loc[(subgroup['Day']==0) | (subgroup['Day']==6)]\n",
    "\n",
    "    \n",
    "    #   \n",
    "        \n",
    "    grouped = subgroup.groupby(['Target', 'AntennaID'])['AntennaID'].agg({'count': np.size})\n",
    "    grouped.reset_index(inplace=True,drop=False)\n",
    "    \n",
    "    del subgroup\n",
    "    #reordeno dentro de c/ target por el count del antenna, esto me sirve para despues ordenar las antenas por uso\n",
    "    grouped.sort_values(by=['Target','count'],ascending=False,inplace=True)\n",
    "    \n",
    "    ##enriquezco la muestra con datos epidemicos\n",
    "    #primero agrego a cada antenna del df el dato de si es epidemica\n",
    "    #despues agrupo por el target y me fijo solo la columna epidemica en c/grupo\n",
    "    #finalmente sumo en c/ grupo y tomo la parte superior \n",
    "#entera de esa division con el largo del grupo. Si uso al menos una antena epidemica entonces esta expuesto(==1) Si no,\n",
    "# da 0 pues no estuvo expuesto.        \n",
    "    ##enriquezco la muestra con datos epidemicos\n",
    "\n",
    "    exposed_info =grouped.join(antennas['EPIDEMIC'], on='AntennaID').\\\n",
    "    groupby('Target')['EPIDEMIC'].\\\n",
    "        agg({'EXPOSED' : lambda x: int( np.ceil(np.sum(x)*1.0/np.size(x)) )}) \n",
    "    \n",
    "    #actualizo la tabla\n",
    "    grouped = grouped.join(exposed_info['EXPOSED'],on=\"Target\")\n",
    "    del exposed_info\n",
    "    \n",
    "    \n",
    "    #creo la tabla filtrada solo por users, que es la que voy a terminar guardando (hay tantos rows como users)\n",
    "    output_table = grouped.drop_duplicates(subset = 'Target', keep='first')\n",
    "    #re indexo\n",
    "    output_table.index = output_table['Target'].values\n",
    "    \n",
    "    #agrupo ahora la tabla por target para hacer todos los calculos en los grupos\n",
    "    \n",
    "    grouped = grouped.groupby('Target')  \n",
    "    \n",
    "    #aca voy a ir agregando las top 10 antennas utilizadas por el user, Si no llego a 10 antennas, relleno con NaNs\n",
    "    for i in range(0,10):\n",
    "        #me quedo con la i-esima fila de c/grupo (si no hay fila, no toma en cuenta ese Target)\n",
    "        \n",
    "        buffer_table = grouped.nth(i)[['AntennaID','count']]\n",
    "        #renombre a iesima AntennaId e iesimo count\n",
    "        buffer_table.columns=['AntennaID_%i'%i,'count_%i'%i]\n",
    "        #agrego esta info como nuevas columnas, dejando lo demas como NaNs\n",
    "        output_table = pd.concat([output_table, buffer_table], axis=1, join_axes=[output_table.index])\n",
    "    \n",
    "    del grouped\n",
    "    #los primeros AntennaIDs ya no me sirven, idem con el primer\n",
    "    output_table.drop('AntennaID', axis=1, inplace=True)\n",
    "    output_table.drop('count', axis=1, inplace=True)\n",
    "    \n",
    "    #para los datos faltantes dentro del Top10, relleno con -1s, que vendrian a ser los NaNs\n",
    "    output_table.fillna(-1,inplace=True)\n",
    "    \n",
    "    #ojo aca que en el caso que entren hashes entonces no los va a poder convertir\n",
    "    output_table = output_table.astype(int,copy=False)\n",
    "    \n",
    "    #agrego info de EPIDEMIC, asumiendo a alguien como epidemico si al antenna donde vive (la AntennaID_0) \n",
    "    #esta catalogada como EPIDEMIC\n",
    "    test_table =test_table.join(antennas['EPIDEMIC'], on='AntennaID_0')\n",
    "    \n",
    "    \n",
    "    #print(output_table.columns)\n",
    "    #print(\"la tabla es:\\n\") \n",
    "    #print(output_table.head(5))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    #print(output_table.dytpes)\n",
    "    #pd.to_numeric(output_table['EXPOSED'])\n",
    "    \n",
    "    #for i in range(3,22):\n",
    "    #    pd.to_numeric(output_table[output_table.columns[i]]) \n",
    "    #print(output_table.dytpes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #aca termino guardando el output final pero solo para esos usuarios % pass ==group\n",
    "    output_table.to_csv(output_file, index = False, \n",
    "                   header = False, mode='a',compression='gzip')\n",
    "then = now\n",
    "seconds = time.time() - then\n",
    "print(\"total running time of script is %d \" % seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#para observar la distribucion de veces que un usuario llama en un mes.\n",
    "tabla['Target'].value_counts()[(tabla['Target'].value_counts()>5) & (tabla['Target'].value_counts() < 500)].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample['Target'].value_counts()[(sample['Target'].value_counts()>5) & (sample['Target'].value_counts() < 500)].hist(bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#con esto extraigo el mapping user --> antennas used that month\n",
    "user_antenna = tabla.groupby('Target')['AntennaID'].apply(lambda x: x.tolist()).to_dict();\n",
    "user_antenna2 = tabla.groupby('Target')['AntennaID'].apply(lambda x: x.tolist()).to_dict();\n",
    "\n",
    "user_antenna_map_concat = defaultdict(list)\n",
    "\n",
    "#Veo como concatenar dos \"maps\" distintos y dejando solo las antenas unicas..\n",
    "for d in (user_antenna_map, user_antenna_map2): # podemos meter cuantos dicts queramos aca..\n",
    "    for key, value in d.iteritems():\n",
    "        #list(set()) elimina repetidos\n",
    "        user_antenna_map_concat[key] = list(set(user_antenna_map_concat[key] + value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.call(\" echo \\\"Hello World \\\" \",shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir(os.getcwd())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(rootdir)\n",
    "os.walk('.').next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(rootdir)\n",
    "print \"testdata\" in os.listdir(os.getcwd()) \n",
    "if \"testdata\" in os.listdir(os.getcwd()):\n",
    "    os.system(\"mkdir testdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

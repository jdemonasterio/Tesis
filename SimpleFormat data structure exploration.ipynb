{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleFormat files exploration and processing\n",
    "La idea es\n",
    "\n",
    "* observar estructura de la tabla \n",
    "* limpiar aquella cosas que no sean utilizadas\n",
    "* descartar todos aquellos usuarios que no sean de la telco pues no pueden proveer info de su geolocalizacion. \n",
    "* Generar un mapping de telco users solamente\n",
    "* Catalogar las antenas como pertenecientes o no a regiones del *\"Gran Chaco\"* mejicano\n",
    "\n",
    "## definiciones utilizadas\n",
    "* timestamp arranca en 0 segundos para 01/01/2012 00:00am\n",
    "* el antennaID esta atado solamente al usuario de la TelCo. Los users de la TelCo van todos en la primer columna (independientemente del nombre de la columna..) entonces si la llamada es entrante quiere decir que el user de la telco esta recibiendo un llamado en esa antennaID. si la llamada es saliente es al reves. (Podria pasar que sea comunicaciones inter-Telco y ahi habria dos records uno saliente y otro entrante pero con los mismos 2 userIDs).\n",
    "* La Direction, viene dada relativa al user de la primer columna (incoming si entra y viceversa si es outgoing).\n",
    "\n",
    "### el dataset vendria con este header\n",
    " {'Target':np.int32 \\\\\\  'Destination':np.uint32 \\\\\\  'TimeStamp':np.uint32 \\\\\\   'Duration':np.uint16 \\\\\\  'AntennaID':np.uint16}\n",
    "\n",
    "por lo cual seria conveniente eliminar los headers de cada .txt.gz para parsear rapidamente, asignar dtypes y despues volver a asignar column names aca en pandas (u otro program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd; import numpy as np; import os;import random;\n",
    "pd.set_option('display.max_rows', 300)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "antennas= pd.read_csv('/home/juan/mobility-study/surrogated_antennas_complete.txt',sep = \"|\",header=0,index_col=0)\n",
    "antennas.index.rename(\"AntennaID\",inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls /grandata/simple_format/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ejemplo  \"/grandata/simple_format/01-02012014.txt.gz\"\n",
    "#seteamos el lugar de trabajo\n",
    "rootdir=\"/grandata/simple_format/\"\n",
    "os.chdir(rootdir)\n",
    "year = \"2015\"; \n",
    "month_start= \"01\";\n",
    "#todos estos sum_links terminan 2 meses mas tardes\n",
    "month_end= str(int(month_start)+2)\n",
    "if int(month_end )<10:\n",
    "    month_end=\"0\"+month_end\n",
    "    \n",
    "day_start = \"01\"\n",
    "day_end = str(int(day_start)+1)\n",
    "if int(day_end )<10:\n",
    "    day_end=\"0\"+ day_end\n",
    "\n",
    "input_file= rootdir +\"simple_format_{y}{ms}.txt.gz\"\\\n",
    "                .format(y=year,ms=month_start,me=month_end,\n",
    "                       ds=day_start,de=day_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_output_file(hash_map=False,night_filter=False,week_end=False):\n",
    "    output = \"/home/juan/mobility-study/output\"\n",
    "    if week_end == True:\n",
    "        output = output + \"_wkend\"\n",
    "    if night_filter == True:\n",
    "        output = output + \"_ngtfilter\"\n",
    "    if hash_map == True:\n",
    "        output = output + \"_user_hash_map\"\n",
    "    return output + \".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER OTHER_USER DIRECTION TIMESTAMP DURATION ANTENNA_ID\r\n",
      "216E0E920C0B51093276E270CC36CB72 5492B0033211A161BA4DCECD83A6690E O 94695418 88 1\r\n",
      "73C2C1F8233884659CC65DF58BA2D031 0ECA1BB365D1BD1F3B54F5F5225A4337 I 94820900 153 2\r\n",
      "73C2C1F8233884659CC65DF58BA2D031 0ECA1BB365D1BD1F3B54F5F5225A4337 O 94842758 189 3\r\n",
      "73C2C1F8233884659CC65DF58BA2D031 0ECA1BB365D1BD1F3B54F5F5225A4337 O 94820507 361 2\r\n",
      "73C2C1F8233884659CC65DF58BA2D031 0ECA1BB365D1BD1F3B54F5F5225A4337 O 94831395 52 4\r\n",
      "73C2C1F8233884659CC65DF58BA2D031 29D24A2CD146FFEDC05757A22BAED052 I 94835642 40 5\r\n",
      "73C2C1F8233884659CC65DF58BA2D031 3386587CF389FB2B195C60D92DEB9F81 I 94711785 25 2\r\n",
      "73C2C1F8233884659CC65DF58BA2D031 3386587CF389FB2B195C60D92DEB9F81 I 94838044 27 6\r\n",
      "73C2C1F8233884659CC65DF58BA2D031 3386587CF389FB2B195C60D92DEB9F81 O 94819123 34 6\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zcat $input_file | head -n10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mappear los user hashes a ints \n",
    "La idea es que para esos 5 meses de telco datos que estamos usando, buscamos el conjunto de usuarios de la telco que efectivamente va a ser parte del analisis de mobilidad (sin todavia descartar a los usuarios por filtros de noche o cantidad de uso celular por ejemplo)\n",
    "\n",
    "Tomamos todo el conjunto de usuarios vistos en los ultimos 5 meses de simple_format y creamos un diccionario que vaya del hash a algun valor en range(1,len(total_num_of_users))\n",
    "\n",
    "update 06/03: trate de leer todos los files juntos pero a veces se rompen. los leo de a uno, mes a mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#levanto la tabla de users hash_map.\n",
    "user_hash_range_map_file = get_output_file(True) + \".gz\"\n",
    "user_hash_range_map = pd.read_csv(user_hash_range_map_file,\n",
    "                                  sep=\"|\",\n",
    "                                  index_col=0,\n",
    "                                  header=0\n",
    "                )\n",
    "\n",
    "users = set(user_hash_range_map.index.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 9-2015 simple_format file \n",
      " \n",
      "\n",
      "reading chunk numb 3 for month 08, time elapsed is 420.625684023\n",
      "reading chunk numb 6 for month 08, time elapsed is 848.168428898\n",
      "total running time of script is 867 \n"
     ]
    }
   ],
   "source": [
    "## script para obtener todos los users en forma de hash.\n",
    "\n",
    "#ver el tiempo que tarda\n",
    "start_time = time.time()\n",
    "\n",
    "if not('users' in globals()):\n",
    "    users = set()\n",
    "antennas_ids = set()\n",
    "#ejemplo  \"/grandata/voice/201401_geo/GEO\\ Voz\\ Pospago\\ 01-02012014.txt.gz\"\n",
    "#seteamos el lugar de trabajo\n",
    "rootdir=\"/grandata/simple_format/\"\n",
    "os.chdir(rootdir)\n",
    "year = \"2015\"; \n",
    "months =  [\"0\"+str(month) for month in range(5,10)]\n",
    "\n",
    "print(\"reading {ms}-{y} simple_format file \\n \\n\".format(y=year, ms=month))\n",
    "month_start= \"09\";\n",
    "month_end= str(int(month_start)+2)\n",
    "if int(month_end )<10:\n",
    "    month_end=\"0\"+month_end\n",
    "\n",
    "day_start = \"01\"\n",
    "day_end = str(int(day_start)+1)\n",
    "if int(day_end )<10:\n",
    "    day_end=\"0\"+ day_end\n",
    "\n",
    "input_file= rootdir +\"simple_format_{y}{ms}.txt.gz\"\\\n",
    "                .format(y=year,ms=month_start,me=month_end,\n",
    "                       ds=day_start,de=day_end)\n",
    "\n",
    "#el chunk basicamente va leyendo el file de a 'chunksize' cantidad de filas\n",
    "#subgroup = pd.DataFrame()\n",
    "#leo de a 40 millones\n",
    "table = pd.read_csv(\n",
    "        input_file,\n",
    "        engine = 'c',\n",
    "        chunksize = 7*10**7,\n",
    "#            iterator =True,\n",
    "        sep = ' ',\n",
    "        index_col=None,\n",
    "        header =0,\n",
    "        #names = ['User', 'Other_User', 'Direction', 'Timestamp'],\n",
    "        #usecols = ['USER','ANTENNA_ID']\n",
    "         usecols = ['USER']   \n",
    "        )\n",
    "\n",
    "#cuando entramos a este loop, table tiene tantos 'chunks' como el valor entero de la cantidad de lineas en el file\n",
    "#dividido el tamanyo del chunksize\n",
    "i=0\n",
    "for chunk in table:\n",
    "    i+=1\n",
    "    if i%3==0:\n",
    "        print(\"reading chunk numb {n} for month {m}, time elapsed is {t}\".format(n=i,m= month_start,\n",
    "                                                                                 t=time.time()-start_time))\n",
    "    #a cada chunk filtro por todos los Targets que tengan resto group modulo passes. y trabajo sobre la tabla\n",
    "    #subgroup nada mas que ahora tiene pocos usuarios\n",
    "    #subgroup = subgroup.append(chunk[chunk['Target'] % passes == group])\n",
    "    users |= set(chunk['USER'].values)\n",
    "    #antennas_ids |= set(chunk['ANTENNA_ID'].values)\n",
    "#entonces la idea es que yo ahora solo voy a trabajar, dentro de la tabla\n",
    "\n",
    "seconds = time.time() - start_time\n",
    "print(\"total running time of script is %d \" % seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### obs1. una corrida de un solo mes tarda aprox 20 mins.\n",
    "### obs2. cada simpleF file tiene aprox 350m de lineas\n",
    "### obs3. usamos chunksize de 40m, solo estoy levantando h/ 7gb de ram. Con chunksize 70m levanta h/11gb de ram \n",
    "### obs4. hay aprox 5.3m de users \n",
    "### obs5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5257613"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#re indexo los usuarios de hash a un mapping de ints\n",
    "vals = range(1, len(users)+1 ) #con +1 porque me gusta pensar que los datos son positivos y los NAn==-1\n",
    "#user_hash_range_map = dict(zip(list(users), vals))\n",
    "users_list = list(users)\n",
    "user_hash_range_map = pd.DataFrame({'User_hash' : users_list,\n",
    "                             'User_int' : vals},\n",
    "                                   #dtype= {'User_int':np.int32}\n",
    "                                  )\n",
    "\n",
    "#user_hash_range_map.sort_values(by='User_int',inplace=True)\n",
    "user_hash_range_map_file = get_output_file(True)\n",
    "#reindexo\n",
    "user_hash_range_map.set_index(['User_hash'],inplace=True)\n",
    "#guardo el file\n",
    "\n",
    "user_hash_range_map.to_csv(path_or_buf=user_hash_range_map_file,\n",
    "                           sep = \"|\"\n",
    "                        )\n",
    "\n",
    "#comprimo el archivo\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "with open(user_hash_range_map_file, 'rb') as f_in, gzip.open(user_hash_range_map_file+'.gz', 'wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "#elimino el file sin comprimir\n",
    "!rm $user_hash_range_map_file    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112M\t/home/juan/mobility-study/output_user_hash_map.txt.gz\r\n"
     ]
    }
   ],
   "source": [
    "!du -ha $user_hash_range_map_file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_hash|User_int\r\n",
      "32BBE907D578C7B8089413AE28C78999|1\r\n",
      "277133E590A0453F0C0657AA573ECC6B|2\r\n",
      "778958CAD23103B25C44C26A0B0CEF0C|3\r\n",
      "7B299D9DC67285D3EA8526CFB835FFF5|4\r\n",
      "5C0E65540627A933C752DA11498BA353|5\r\n",
      "028FDC09FE7C3B843112371D84EF7D54|6\r\n",
      "62F0FB22E0724D4914AE500E4C979354|7\r\n",
      "DFEFFF17DFF3C8A95B42090B273E298C|8\r\n",
      "FEB22A2B9AFF2E9489A6383AFC270693|9\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#chequeo que este bien escrito\n",
    "!zcat $user_hash_range_map_file* | head -n10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "a ver que seria mas rapido si mapear todos los valores desde hash a int y re indexar \n",
    "o si tomar un filtro basado en si el ultimo caracter es un numero o una letra.\n",
    "\n",
    "Result: Claramente es mejor tomar el filtro directamente sobre la tabla y sin reindexar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "test_table = pd.read_csv(\n",
    "        input_file,\n",
    "        engine = 'c',\n",
    "#            iterator =True,\n",
    "        sep = ' ',\n",
    "        header =0,\n",
    "        nrows = 3*10**6,\n",
    "        #names = ['User', 'Other_User', 'Direction', 'Timestamp'],\n",
    "        #usecols = ['USER','ANTENNA_ID']\n",
    "         usecols = ['USER']   \n",
    "        )\n",
    "test_hash = user_hash_range_map.sample(frac=1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.37477016448975"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#este es el reindexing a correr.\n",
    "#user_hash_range_map_dict = user_hash_range_map.to_dict()['User_int']\n",
    "\n",
    "timeit.timeit('test_table[\\'User_int\\']= test_table[\\'USER\\'].map(user_hash_range_map_dict)',\n",
    "              \"from __main__ import test_table, user_hash_range_map_dict\",\n",
    "              number=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filtra en base a una lista de caracteres que uno le pida al hash\n",
    "#toma como entrada una columna y la devuelve convertida en bool\n",
    "#test_table2 = test_table['USER'].apply(lambda x: x[-1] in ['1','2','3'] )\n",
    "\n",
    "timeit.timeit('test_table2 = test_table[\\'USER\\'].apply(lambda x: x[-1] in [\\'1\\',\\'2\\',\\'3\\'] )',\n",
    "              \"from __main__ import test_table\",\n",
    "              number=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[l[i:i+n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#este seria el filtro.\n",
    "test_table2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#paso de segundos a horas\n",
    "    #notar que timestamp arranca en 0 segundos para domingo 01/01/2012 00:00am \n",
    "    #con lo cual domingo es el dia 0, lunes el 1, asi..\n",
    "    \n",
    "    subgroup['Hour'] =  (subgroup['TimeStamp'].values*1.0/3600)%24\n",
    "    subgroup['Day']  =  (subgroup['TimeStamp'].values*1.0/(3600*24))%7\n",
    "    \n",
    "    #filtro usuarios con pocos o demasiados llamados en general menos de 5 mensuales y mas de 400  \n",
    "    insignificant_users = subgroup['Target'].value_counts()[(subgroup['Target'].value_counts() < 5) \\\n",
    "                        | (subgroup['Target'].value_counts() > 400)].index.values.tolist()\n",
    "    subgroup= subgroup.loc[~subgroup['Target'].isin(insignificant_users)]\n",
    "    \n",
    "    \n",
    "    #filtro segun nightfilter y week_end\n",
    "    if night_filter == True:\n",
    "        subgroup = subgroup.loc[(subroup['Hour']<8) | (subgroup['Hour']>18)]\n",
    "    if week_end == True:\n",
    "        subgroup = subgroup.loc[(subgroup['Day']==0) | (subgroup['Day']==6)]\n",
    "\n",
    "    \n",
    "    #   \n",
    "        \n",
    "    grouped = subgroup.groupby(['Target', 'AntennaID'])['AntennaID'].agg({'count': np.size})\n",
    "    grouped.reset_index(inplace=True,drop=False)\n",
    "    \n",
    "    del subgroup\n",
    "    #reordeno dentro de c/ target por el count del antenna, esto me sirve para despues ordenar las antenas por uso\n",
    "    grouped.sort_values(by=['Target','count'],ascending=False,inplace=True)\n",
    "    \n",
    "    ##enriquezco la muestra con datos epidemicos\n",
    "    #primero agrego a cada antenna del df el dato de si es epidemica\n",
    "    #despues agrupo por el target y me fijo solo la columna epidemica en c/grupo\n",
    "    #finalmente sumo en c/ grupo y tomo la parte superior \n",
    "#entera de esa division con el largo del grupo. Si uso al menos una antena epidemica entonces esta expuesto(==1) Si no,\n",
    "# da 0 pues no estuvo expuesto.        \n",
    "    ##enriquezco la muestra con datos epidemicos\n",
    "\n",
    "    exposed_info =grouped.join(antennas['EPIDEMIC'], on='AntennaID').\\\n",
    "    groupby('Target')['EPIDEMIC'].\\\n",
    "        agg({'EXPOSED' : lambda x: int( np.ceil(np.sum(x)*1.0/np.size(x)) )}) \n",
    "    \n",
    "    #actualizo la tabla\n",
    "    grouped = grouped.join(exposed_info['EXPOSED'],on=\"Target\")\n",
    "    del exposed_info\n",
    "    \n",
    "    \n",
    "    #creo la tabla filtrada solo por users, que es la que voy a terminar guardando (hay tantos rows como users)\n",
    "    output_table = grouped.drop_duplicates(subset = 'Target', keep='first')\n",
    "    #re indexo\n",
    "    output_table.index = output_table['Target'].values\n",
    "    \n",
    "    #agrupo ahora la tabla por target para hacer todos los calculos en los grupos\n",
    "    \n",
    "    grouped = grouped.groupby('Target')  \n",
    "    \n",
    "    #aca voy a ir agregando las top 10 antennas utilizadas por el user, Si no llego a 10 antennas, relleno con NaNs\n",
    "    for i in range(0,10):\n",
    "        #me quedo con la i-esima fila de c/grupo (si no hay fila, no toma en cuenta ese Target)\n",
    "        \n",
    "        buffer_table = grouped.nth(i)[['AntennaID','count']]\n",
    "        #renombre a iesima AntennaId e iesimo count\n",
    "        buffer_table.columns=['AntennaID_%i'%i,'count_%i'%i]\n",
    "        #agrego esta info como nuevas columnas, dejando lo demas como NaNs\n",
    "        output_table = pd.concat([output_table, buffer_table], axis=1, join_axes=[output_table.index])\n",
    "    \n",
    "    del grouped\n",
    "    #los primeros AntennaIDs ya no me sirven, idem con el primer\n",
    "    output_table.drop('AntennaID', axis=1, inplace=True)\n",
    "    output_table.drop('count', axis=1, inplace=True)\n",
    "    \n",
    "    #para los datos faltantes dentro del Top10, relleno con -1s, que vendrian a ser los NaNs\n",
    "    output_table.fillna(-1,inplace=True)\n",
    "    \n",
    "    #ojo aca que en el caso que entren hashes entonces no los va a poder convertir\n",
    "    output_table = output_table.astype(int,copy=False)\n",
    "    \n",
    "    #agrego info de EPIDEMIC, asumiendo a alguien como epidemico si al antenna donde vive (la AntennaID_0) \n",
    "    #esta catalogada como EPIDEMIC\n",
    "    test_table =test_table.join(antennas['EPIDEMIC'], on='AntennaID_0')\n",
    "    \n",
    "    \n",
    "    #print(output_table.columns)\n",
    "    #print(\"la tabla es:\\n\") \n",
    "    #print(output_table.head(5))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    #print(output_table.dytpes)\n",
    "    #pd.to_numeric(output_table['EXPOSED'])\n",
    "    \n",
    "    #for i in range(3,22):\n",
    "    #    pd.to_numeric(output_table[output_table.columns[i]]) \n",
    "    #print(output_table.dytpes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #aca termino guardando el output final pero solo para esos usuarios % pass ==group\n",
    "    output_table.to_csv(output_file, index = False, \n",
    "                   header = False, mode='a',compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#numero de usuarios en esta tabla\n",
    "len(np.unique(tabla2.Target.values)), len(np.unique(tabla.Target.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tabla.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#agarro una muestra de 300k para hacer pruebas\n",
    "sample=tabla.ix[random.sample(tabla.index, 3*(10**5))]\n",
    "sample2 = tabla.ix[random.sample(tabla.index, 3*(10**5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#enriquezco el dataset con la info de antennas_mexico.csv\n",
    "enriched_sample = pd.merge(sample,antennas,left_on=\"AntennaID\",right_index=True)\n",
    "enriched_sample2 = pd.merge(sample2,antennas,left_on=\"AntennaID\",right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enriched_sample.sort_values(by='TimeStamp',ascending=True,inplace=True)\n",
    "enriched_sample2.sort_values(by='TimeStamp',ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enriched_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Mobility\n",
    "La idea es ahora mirar dentro de la base (prototipamos con sample) la lista de antenas utilizadas por cada usuario para obtener la variable dependiente $Y$ que vendria a ser si el tipo viajo o no por la zona endemica (visto como un llamado basicamente desde una antena en la epidemic zone). Despues buscaremos atributos, o variables regresoras que logren predecir ese comportamiento en el pasado pero usando solo los atributos en un timeframe \"del futuro\". Busco atributos que correlacionen con 'haber viajado por la zona endemica en el pasado'\n",
    "#### pseudocode:\n",
    "* tomamos el set de origins/targets de la tabla.\n",
    "* p/c/user lo mapeamos a un conjunto de antenas que sabemos que el tipo uso en algun CDR. Aca usamos alguna operacion split, apply, combine dondeset donde el key va a ser la columna origin y la data es AntennaID. Ahi extraemos la lista de antenas que el user uso. \n",
    "* descartamos del dataset los users \"vacios\". i.e. aquellos que su conjunto de antenas da vacio. Para esto descartamos todas las filas que no tienen ningun user (origin o target) en la tabla split/apply/combine. Asi obtendremos una lista de usuarios de la telco que viajaron por mexico a partir de sus llamados en ese mes. Despues solo hacer un outer join de todas estas tablas resultantes para cada mes. Asi como tambien un join de estos \"sets\" de antennas por user. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = tabla['AntennaID'].groupby(tabla['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#con esto extraigo el mapping user --> antennas used that month\n",
    "user_antenna_map = sample.groupby('Target')['AntennaID'].apply(lambda x: x.tolist()).to_dict();\n",
    "user_antenna_map2 = sample2.groupby('Target')['AntennaID'].apply(lambda x: x.tolist()).to_dict();\n",
    "\n",
    "#esto es 'tipo' un head para dicts\n",
    "dict(user_antenna_map.items()[0:10]), dict(user_antenna_map2.items()[0:10])\n",
    "\n",
    "from collections import defaultdict\n",
    "user_antenna_map_concat = defaultdict(list)\n",
    "\n",
    "#Veo como concatenar dos \"maps\" distintos y dejando solo las antenas unicas..\n",
    "for d in (user_antenna_map, user_antenna_map2): # podemos meter cuantos dicts queramos aca..\n",
    "    for key, value in d.iteritems():\n",
    "        #list(set()) elimina repetidos\n",
    "        user_antenna_map_concat[key] = list(set(user_antenna_map_concat[key] + value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#para observar la distribucion de veces que un usuario llama en un mes.\n",
    "tabla['Target'].value_counts()[(tabla['Target'].value_counts()>5) & (tabla['Target'].value_counts() < 500)].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample['Target'].value_counts()[(sample['Target'].value_counts()>5) & (sample['Target'].value_counts() < 500)].hist(bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#con esto extraigo el mapping user --> antennas used that month\n",
    "user_antenna = tabla.groupby('Target')['AntennaID'].apply(lambda x: x.tolist()).to_dict();\n",
    "user_antenna2 = tabla.groupby('Target')['AntennaID'].apply(lambda x: x.tolist()).to_dict();\n",
    "\n",
    "user_antenna_map_concat = defaultdict(list)\n",
    "\n",
    "#Veo como concatenar dos \"maps\" distintos y dejando solo las antenas unicas..\n",
    "for d in (user_antenna_map, user_antenna_map2): # podemos meter cuantos dicts queramos aca..\n",
    "    for key, value in d.iteritems():\n",
    "        #list(set()) elimina repetidos\n",
    "        user_antenna_map_concat[key] = list(set(user_antenna_map_concat[key] + value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.call(\" echo \\\"Hello World \\\" \",shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir(os.getcwd())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(rootdir)\n",
    "os.walk('.').next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(rootdir)\n",
    "print \"testdata\" in os.listdir(os.getcwd()) \n",
    "if \"testdata\" in os.listdir(os.getcwd()):\n",
    "    os.system(\"mkdir testdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
